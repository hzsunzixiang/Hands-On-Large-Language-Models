"""
Chapter 2 - Tokens å’Œ Token Embeddings
æ¢ç´¢ tokens å’Œ embeddings ä½œä¸ºæ„å»º LLM çš„é‡è¦ç»„æˆéƒ¨åˆ†

ä¸»è¦å†…å®¹ï¼š
1. Tokenizer åŸºç¡€ - æ–‡æœ¬å¦‚ä½•è¢«åˆ‡åˆ†å’Œç¼–ç 
2. æ¯”è¾ƒä¸åŒæ¨¡å‹çš„ Tokenizer
3. ä¸Šä¸‹æ–‡è¯åµŒå…¥ (Contextualized Word Embeddings)
4. å¥å­/æ–‡æ¡£åµŒå…¥ (Sentence Embeddings)
5. ä¼ ç»Ÿè¯åµŒå…¥ (Word2Vec/GloVe)
6. å®æˆ˜ï¼šåŸºäºåµŒå…¥çš„æ­Œæ›²æ¨èç³»ç»Ÿ
"""

import torch
import numpy as np


def get_device():
    """æ£€æµ‹å¯ç”¨è®¾å¤‡"""
    if torch.cuda.is_available():
        return "cuda"
    elif torch.backends.mps.is_available():
        return "mps"
    return "cpu"


# ============================================================
# Part 1: Tokenizer åŸºç¡€
# ============================================================
def demo_tokenizer_basics():
    """æ¼”ç¤º Tokenizer çš„åŸºæœ¬ä½¿ç”¨"""
    print("\n" + "=" * 60)
    print("Part 1: Tokenizer åŸºç¡€")
    print("=" * 60)
    
    from transformers import AutoTokenizer
    
    # åŠ è½½ Phi-3 çš„ tokenizer
    tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-3-mini-4k-instruct")
    
    # æµ‹è¯•æ–‡æœ¬
    prompt = "Write an email apologizing to Sarah for the tragic gardening mishap."
    
    # å°†æ–‡æœ¬è½¬æ¢ä¸º token IDs
    input_ids = tokenizer(prompt, return_tensors="pt").input_ids
    print(f"\nåŸå§‹æ–‡æœ¬: {prompt}")
    print(f"\nToken IDs shape: {input_ids.shape}")
    print(f"Token IDs: {input_ids[0].tolist()}")
    
    # é€ä¸ªè§£ç  token æŸ¥çœ‹åˆ†è¯ç»“æœ
    print("\nåˆ†è¯ç»“æœ:")
    for i, token_id in enumerate(input_ids[0]):
        token = tokenizer.decode(token_id)
        print(f"  {i}: ID={token_id.item():5d} -> '{token}'")
    
    # æ¼”ç¤ºå­è¯ç»„åˆ
    print("\nå­è¯ç»„åˆç¤ºä¾‹:")
    print(f"  tokenizer.decode([3323, 622]) = '{tokenizer.decode([3323, 622])}'")
    
    return tokenizer


# ============================================================
# Part 2: æ¯”è¾ƒä¸åŒæ¨¡å‹çš„ Tokenizer
# ============================================================
def demo_compare_tokenizers():
    """æ¯”è¾ƒä¸åŒ LLM çš„åˆ†è¯æ–¹å¼"""
    print("\n" + "=" * 60)
    print("Part 2: æ¯”è¾ƒä¸åŒæ¨¡å‹çš„ Tokenizer")
    print("=" * 60)
    
    from transformers import AutoTokenizer
    
    # ANSI é¢œè‰²ä»£ç ç”¨äºå¯è§†åŒ–
    colors_list = [
        '102;194;165', '252;141;98', '141;160;203',
        '231;138;195', '166;216;84', '255;217;47'
    ]
    
    def show_tokens(sentence, tokenizer_name):
        """å¯è§†åŒ–å±•ç¤ºåˆ†è¯ç»“æœ"""
        try:
            tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
            token_ids = tokenizer(sentence).input_ids
            print(f"\n{tokenizer_name} ({len(token_ids)} tokens):")
            tokens = []
            for idx, t in enumerate(token_ids):
                color = colors_list[idx % len(colors_list)]
                token = tokenizer.decode(t)
                tokens.append(f'\x1b[0;30;48;2;{color}m{token}\x1b[0m')
            print(' '.join(tokens))
        except Exception as e:
            print(f"\n{tokenizer_name}: åŠ è½½å¤±è´¥ - {e}")
    
    # æµ‹è¯•æ–‡æœ¬ï¼ˆåŒ…å«å„ç§ç‰¹æ®Šæƒ…å†µï¼‰
    text = """
English and CAPITALIZATION
ğŸµ é¸Ÿ
show_tokens False None elif == >= else:
12.0*50=600
"""
    
    print(f"æµ‹è¯•æ–‡æœ¬: {text}")
    
    # æ¯”è¾ƒä¸åŒçš„ tokenizer
    tokenizers_to_compare = [
        "bert-base-uncased",      # BERT (å°å†™)
        "bert-base-cased",        # BERT (ä¿ç•™å¤§å°å†™)
        "gpt2",                   # GPT-2
        "google/flan-t5-small",   # T5
    ]
    
    for tokenizer_name in tokenizers_to_compare:
        show_tokens(text, tokenizer_name)


# ============================================================
# Part 3: ä¸Šä¸‹æ–‡è¯åµŒå…¥ (Contextualized Embeddings)
# ============================================================
def demo_contextualized_embeddings():
    """æ¼”ç¤ºä»è¯­è¨€æ¨¡å‹è·å–ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„è¯åµŒå…¥"""
    print("\n" + "=" * 60)
    print("Part 3: ä¸Šä¸‹æ–‡è¯åµŒå…¥ (Contextualized Embeddings)")
    print("=" * 60)
    
    from transformers import AutoModel, AutoTokenizer
    
    # åŠ è½½ DeBERTa æ¨¡å‹å’Œ tokenizer
    print("\nåŠ è½½ DeBERTa æ¨¡å‹...")
    tokenizer = AutoTokenizer.from_pretrained("microsoft/deberta-base")
    model = AutoModel.from_pretrained("microsoft/deberta-v3-xsmall")
    
    # å¤„ç†æ–‡æœ¬
    text = "Hello world"
    tokens = tokenizer(text, return_tensors='pt')
    
    # è·å–ä¸Šä¸‹æ–‡åµŒå…¥
    with torch.no_grad():
        output = model(**tokens)[0]
    
    print(f"\nè¾“å…¥æ–‡æœ¬: '{text}'")
    print(f"Token æ•°é‡: {output.shape[1]}")
    print(f"åµŒå…¥ç»´åº¦: {output.shape[2]}")
    print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")  # [batch, tokens, embedding_dim]
    
    # å±•ç¤ºæ¯ä¸ª token çš„ä¿¡æ¯
    print("\næ¯ä¸ª Token çš„åµŒå…¥:")
    for i, token_id in enumerate(tokens['input_ids'][0]):
        token = tokenizer.decode(token_id)
        embedding = output[0, i, :5].tolist()  # åªæ˜¾ç¤ºå‰5ç»´
        print(f"  {i}: '{token}' -> [{', '.join(f'{x:.4f}' for x in embedding)}, ...]")


# ============================================================
# Part 4: å¥å­åµŒå…¥ (Sentence Embeddings)
# ============================================================
def demo_sentence_embeddings():
    """æ¼”ç¤ºä½¿ç”¨ Sentence Transformers ç”Ÿæˆå¥å­åµŒå…¥"""
    print("\n" + "=" * 60)
    print("Part 4: å¥å­åµŒå…¥ (Sentence Embeddings)")
    print("=" * 60)
    
    try:
        from sentence_transformers import SentenceTransformer
    except ImportError:
        print("è¯·å®‰è£…: pip install sentence-transformers")
        return
    
    # åŠ è½½å¥å­åµŒå…¥æ¨¡å‹
    print("\nåŠ è½½ Sentence Transformer æ¨¡å‹...")
    model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')
    
    # æµ‹è¯•å¥å­
    sentences = [
        "Best movie ever!",
        "This film is amazing!",
        "I love programming in Python.",
        "The weather is nice today."
    ]
    
    # ç”ŸæˆåµŒå…¥
    embeddings = model.encode(sentences)
    
    print(f"\nå¥å­æ•°é‡: {len(sentences)}")
    print(f"åµŒå…¥ç»´åº¦: {embeddings.shape[1]}")
    
    # è®¡ç®—ç›¸ä¼¼åº¦
    from sklearn.metrics.pairwise import cosine_similarity
    
    print("\nå¥å­ç›¸ä¼¼åº¦çŸ©é˜µ:")
    similarity_matrix = cosine_similarity(embeddings)
    
    # æ‰“å°ç›¸ä¼¼åº¦
    for i, s1 in enumerate(sentences):
        print(f"\n'{s1[:30]}...' ä¸å…¶ä»–å¥å­çš„ç›¸ä¼¼åº¦:")
        for j, s2 in enumerate(sentences):
            if i != j:
                print(f"  -> '{s2[:30]}...': {similarity_matrix[i][j]:.4f}")


# ============================================================
# Part 5: ä¼ ç»Ÿè¯åµŒå…¥ (Word2Vec/GloVe)
# ============================================================
def demo_word_embeddings():
    """æ¼”ç¤ºä¼ ç»Ÿè¯åµŒå…¥çš„ä½¿ç”¨"""
    print("\n" + "=" * 60)
    print("Part 5: ä¼ ç»Ÿè¯åµŒå…¥ (GloVe)")
    print("=" * 60)
    
    try:
        import gensim.downloader as api
    except ImportError:
        print("è¯·å®‰è£…: pip install gensim")
        return
    
    # ä¸‹è½½ GloVe è¯åµŒå…¥ï¼ˆçº¦ 66MBï¼‰
    print("\nä¸‹è½½ GloVe è¯åµŒå…¥ (glove-wiki-gigaword-50)...")
    print("é¦–æ¬¡ä¸‹è½½çº¦ 66MBï¼Œè¯·ç¨å€™...")
    model = api.load("glove-wiki-gigaword-50")
    
    # æŸ¥æ‰¾ç›¸ä¼¼è¯
    word = "king"
    print(f"\nä¸ '{word}' æœ€ç›¸ä¼¼çš„è¯:")
    similar_words = model.most_similar([model[word]], topn=10)
    for word, score in similar_words:
        print(f"  {word}: {score:.4f}")
    
    # è¯å‘é‡è¿ç®—: king - man + woman â‰ˆ queen
    print("\nè¯å‘é‡è¿ç®—: king - man + woman = ?")
    result = model.most_similar(positive=['king', 'woman'], negative=['man'], topn=3)
    for word, score in result:
        print(f"  {word}: {score:.4f}")


# ============================================================
# Part 6: å®æˆ˜ - åŸºäºåµŒå…¥çš„æ­Œæ›²æ¨èç³»ç»Ÿ
# ============================================================
def demo_song_recommendation():
    """ä½¿ç”¨ Word2Vec æ„å»ºæ­Œæ›²æ¨èç³»ç»Ÿ"""
    print("\n" + "=" * 60)
    print("Part 6: åŸºäºåµŒå…¥çš„æ­Œæ›²æ¨èç³»ç»Ÿ")
    print("=" * 60)
    
    import pandas as pd
    from urllib import request
    from gensim.models import Word2Vec
    
    print("\nä¸‹è½½æ’­æ”¾åˆ—è¡¨æ•°æ®...")
    
    # è·å–æ’­æ”¾åˆ—è¡¨æ•°æ®
    data = request.urlopen('https://storage.googleapis.com/maps-premium/dataset/yes_complete/train.txt')
    lines = data.read().decode("utf-8").split('\n')[2:]
    playlists = [s.rstrip().split() for s in lines if len(s.split()) > 1]
    
    # è·å–æ­Œæ›²å…ƒæ•°æ®
    songs_file = request.urlopen('https://storage.googleapis.com/maps-premium/dataset/yes_complete/song_hash.txt')
    songs_file = songs_file.read().decode("utf-8").split('\n')
    songs = [s.rstrip().split('\t') for s in songs_file]
    songs_df = pd.DataFrame(data=songs, columns=['id', 'title', 'artist'])
    songs_df = songs_df.set_index('id')
    
    print(f"æ’­æ”¾åˆ—è¡¨æ•°é‡: {len(playlists)}")
    print(f"æ­Œæ›²æ•°é‡: {len(songs_df)}")
    
    # è®­ç»ƒ Word2Vec æ¨¡å‹
    # æŠŠæ­Œæ›² ID å½“ä½œ "è¯"ï¼Œæ’­æ”¾åˆ—è¡¨å½“ä½œ "å¥å­"
    print("\nè®­ç»ƒ Word2Vec æ¨¡å‹...")
    model = Word2Vec(
        playlists,
        vector_size=32,   # åµŒå…¥ç»´åº¦
        window=20,        # ä¸Šä¸‹æ–‡çª—å£
        negative=50,      # è´Ÿé‡‡æ ·
        min_count=1,
        workers=4
    )
    
    def get_recommendations(song_id, topn=5):
        """è·å–æ­Œæ›²æ¨è"""
        similar_songs = np.array(
            model.wv.most_similar(positive=str(song_id), topn=topn)
        )[:, 0]
        return songs_df.loc[similar_songs]
    
    # æµ‹è¯•æ¨è
    test_song_id = 2172
    print(f"\næµ‹è¯•æ­Œæ›² (ID={test_song_id}):")
    print(songs_df.loc[str(test_song_id)])
    
    print(f"\næ¨èæ­Œæ›²:")
    recommendations = get_recommendations(test_song_id)
    print(recommendations)
    
    # å¦ä¸€ä¸ªæµ‹è¯•
    test_song_id2 = 842
    print(f"\n\næµ‹è¯•æ­Œæ›² (ID={test_song_id2}):")
    print(songs_df.loc[str(test_song_id2)])
    
    print(f"\næ¨èæ­Œæ›²:")
    recommendations2 = get_recommendations(test_song_id2)
    print(recommendations2)


# ============================================================
# ä¸»ç¨‹åº
# ============================================================
def main():
    print("=" * 60)
    print("Chapter 2: Tokens å’Œ Token Embeddings")
    print("=" * 60)
    
    device = get_device()
    print(f"PyTorch ç‰ˆæœ¬: {torch.__version__}")
    print(f"è¿è¡Œè®¾å¤‡: {device}")
    
    # è¿è¡Œå„ä¸ªæ¼”ç¤º
    # å¯ä»¥æ³¨é‡Šæ‰ä¸æƒ³è¿è¡Œçš„éƒ¨åˆ†
    
    # Part 1: Tokenizer åŸºç¡€
    demo_tokenizer_basics()
    
    # Part 2: æ¯”è¾ƒä¸åŒ Tokenizer
    demo_compare_tokenizers()
    
    # Part 3: ä¸Šä¸‹æ–‡è¯åµŒå…¥
    demo_contextualized_embeddings()
    
    # Part 4: å¥å­åµŒå…¥ (éœ€è¦ sentence-transformers)
    demo_sentence_embeddings()
    
    # Part 5: ä¼ ç»Ÿè¯åµŒå…¥ (éœ€è¦ä¸‹è½½æ•°æ®ï¼Œçº¦ 66MB)
    # demo_word_embeddings()
    
    # Part 6: æ­Œæ›²æ¨è (éœ€è¦ä¸‹è½½æ•°æ®å’Œè®­ç»ƒ)
    # demo_song_recommendation()
    
    print("\n" + "=" * 60)
    print("æ¼”ç¤ºå®Œæˆ!")
    print("=" * 60)


if __name__ == "__main__":
    main()
