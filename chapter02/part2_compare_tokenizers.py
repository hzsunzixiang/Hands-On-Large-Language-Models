"""
Part 2: æ¯”è¾ƒä¸åŒæ¨¡å‹çš„ Tokenizer
å±•ç¤ºä¸åŒ LLM çš„åˆ†è¯æ–¹å¼å·®å¼‚
"""
import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"

from transformers import AutoTokenizer

print("=" * 60)
print("Part 2: æ¯”è¾ƒä¸åŒæ¨¡å‹çš„ Tokenizer")
print("=" * 60)

# ANSI é¢œè‰²ä»£ç ç”¨äºå¯è§†åŒ–
colors_list = [
    '102;194;165', '252;141;98', '141;160;203',
    '231;138;195', '166;216;84', '255;217;47'
]

def show_tokens(sentence, tokenizer_name):
    """å¯è§†åŒ–å±•ç¤ºåˆ†è¯ç»“æœ"""
    try:
        tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
        token_ids = tokenizer(sentence).input_ids
        print(f"\n{tokenizer_name} ({len(token_ids)} tokens):")
        tokens = []
        for idx, t in enumerate(token_ids):
            color = colors_list[idx % len(colors_list)]
            token = tokenizer.decode(t)
            tokens.append(f'\x1b[0;30;48;2;{color}m{token}\x1b[0m')
        print(' '.join(tokens))
        
        # ä¹Ÿæ‰“å°çº¯æ–‡æœ¬ç‰ˆæœ¬
        print("  Tokens:", [tokenizer.decode(t) for t in token_ids])
    except Exception as e:
        print(f"\n{tokenizer_name}: åŠ è½½å¤±è´¥ - {e}")

# æµ‹è¯•æ–‡æœ¬ï¼ˆåŒ…å«å„ç§ç‰¹æ®Šæƒ…å†µï¼‰
text = """
English and CAPITALIZATION
ğŸµ é¸Ÿ
show_tokens False None elif == >= else:
12.0*50=600
"""

print(f"\næµ‹è¯•æ–‡æœ¬: {text}")
print("-" * 60)

# æ¯”è¾ƒä¸åŒçš„ tokenizer
tokenizers_to_compare = [
    "bert-base-uncased",      # BERT (å°å†™)
    "bert-base-cased",        # BERT (ä¿ç•™å¤§å°å†™)
    "gpt2",                   # GPT-2
    "google/flan-t5-small",   # T5
]

for tokenizer_name in tokenizers_to_compare:
    show_tokens(text, tokenizer_name)

print("\n" + "=" * 60)
print("è§‚å¯Ÿè¦ç‚¹:")
print("=" * 60)
print("1. bert-base-uncased ä¼šå°†æ‰€æœ‰å­—æ¯è½¬ä¸ºå°å†™")
print("2. ä¸åŒæ¨¡å‹å¯¹ emojiã€ä¸­æ–‡ã€ä»£ç çš„å¤„ç†æ–¹å¼ä¸åŒ")
print("3. æ•°å­¦è¡¨è¾¾å¼çš„åˆ†è¯æ–¹å¼å·®å¼‚å¾ˆå¤§")
print("4. Token æ•°é‡å·®å¼‚åæ˜ äº†è¯è¡¨è®¾è®¡çš„ä¸åŒ")

print("\n" + "=" * 60)
print("Part 2 å®Œæˆ!")
print("=" * 60)
