"""10.3 Multiple Negatives Ranking Loss (MNRL) è®­ç»ƒ Embedding æ¨¡å‹
ä»…ä½¿ç”¨ entailment å¯¹ (premiseâ†’hypothesis) ä½œä¸ºæ­£æ ·æœ¬ï¼Œ
batch å†…å…¶ä»–æ ·æœ¬çš„ hypothesis ä½œä¸º in-batch negativesï¼Œ
å†é¢å¤–æ„é€  soft negative (æ‰“ä¹±çš„ hypothesis)ã€‚
MNRL æ˜¯ç›®å‰æœ€æœ‰æ•ˆçš„ embedding è®­ç»ƒæŸå¤±ä¹‹ä¸€ã€‚
"""
import gc
import random
import time
import torch
from tqdm import tqdm
from datasets import Dataset, load_dataset
from sentence_transformers import SentenceTransformer, losses
from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator
from sentence_transformers.trainer import SentenceTransformerTrainer
from sentence_transformers.training_args import SentenceTransformerTrainingArguments

# ============================================================
# 1. æ•°æ®å‡†å¤‡ â€” æ„é€  (anchor, positive, negative) ä¸‰å…ƒç»„
# ============================================================
total_start = time.time()

print("=" * 60)
print("1. æ„é€ ä¸‰å…ƒç»„æ•°æ® (anchor, positive, soft_negative)")
print("=" * 60)

# Load MNLI, åªä¿ç•™ entailment (label=0)
mnli = load_dataset("glue", "mnli", split="train").select(range(50_000))
mnli = mnli.remove_columns("idx")
mnli = mnli.filter(lambda x: True if x['label'] == 0 else False)
print(f"Entailment æ ·æœ¬æ•°: {len(mnli)}")

# Prepare data and add a soft negative (æ‰“ä¹±çš„ hypothesis)
train_dataset = {"anchor": [], "positive": [], "negative": []}
# ğŸ”§ ä¿®å¤ï¼šå…ˆè½¬æ¢ä¸º Python åˆ—è¡¨ï¼Œå†æ‰“ä¹±
soft_negatives = list(mnli["hypothesis"])  # è½¬æ¢ä¸º Python åˆ—è¡¨
random.shuffle(soft_negatives)             # ç°åœ¨å¯ä»¥å®‰å…¨æ‰“ä¹±

for row, soft_negative in tqdm(zip(mnli, soft_negatives)):
    train_dataset["anchor"].append(row["premise"])
    train_dataset["positive"].append(row["hypothesis"])
    train_dataset["negative"].append(soft_negative)

train_dataset = Dataset.from_dict(train_dataset)
print(f"è®­ç»ƒé›†å¤§å°: {len(train_dataset)}")
print(f"ç¤ºä¾‹:")
print(f"  anchor:   {train_dataset[0]['anchor'][:80]}...")
print(f"  positive: {train_dataset[0]['positive'][:80]}...")
print(f"  negative: {train_dataset[0]['negative'][:80]}...")

# ============================================================
# 2. è¯„ä¼°å™¨ â€” STS-B
# ============================================================
print("\n" + "=" * 60)
print("2. åˆ›å»º STS-B è¯„ä¼°å™¨")
print("=" * 60)

val_sts = load_dataset('glue', 'stsb', split='validation')
evaluator = EmbeddingSimilarityEvaluator(
    sentences1=val_sts["sentence1"],
    sentences2=val_sts["sentence2"],
    scores=[score/5 for score in val_sts["label"]],
    main_similarity="cosine"
)

# ============================================================
# 3. æ¨¡å‹ + MNRL è®­ç»ƒ
# ============================================================
print("\n" + "=" * 60)
print("3. MultipleNegativesRankingLoss è®­ç»ƒ")
print("=" * 60)
train_start = time.time()

embedding_model = SentenceTransformer('bert-base-uncased')
train_loss = losses.MultipleNegativesRankingLoss(model=embedding_model)

args = SentenceTransformerTrainingArguments(
    output_dir="mnrloss_embedding_model",
    num_train_epochs=1,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    warmup_steps=100,
    fp16=False,  # MPS (Apple Silicon) ä¸æ”¯æŒ fp16
    eval_steps=100,
    logging_steps=100,
    report_to="none",
)

trainer = SentenceTransformerTrainer(
    model=embedding_model,
    args=args,
    train_dataset=train_dataset,
    loss=train_loss,
    evaluator=evaluator
)
trainer.train()
train_elapsed = time.time() - train_start
print(f"\nè®­ç»ƒè€—æ—¶: {train_elapsed:.1f}s ({train_elapsed/60:.1f}min)")

# ============================================================
# 4. è¯„ä¼°
# ============================================================
print("\n" + "=" * 60)
print("4. STS-B è¯„ä¼°ç»“æœ")
print("=" * 60)

results = evaluator(embedding_model)
for k, v in results.items():
    print(f"  {k}: {v:.4f}")

# ============================================================
# æ¸…ç†æ˜¾å­˜
# ============================================================
total_elapsed = time.time() - total_start
print(f"\næ€»è¿è¡Œæ—¶é—´: {total_elapsed:.1f}s ({total_elapsed/60:.1f}min)")

gc.collect()
if torch.cuda.is_available():
    torch.cuda.empty_cache()
elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
    torch.mps.empty_cache()
print("\næ˜¾å­˜å·²æ¸…ç†")
