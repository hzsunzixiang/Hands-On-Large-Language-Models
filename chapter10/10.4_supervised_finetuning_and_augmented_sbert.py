"""10.4 æœ‰ç›‘ç£å¾®è°ƒ + Augmented SBERT
Part A: åœ¨é¢„è®­ç»ƒçš„ all-MiniLM-L6-v2 ä¸Šç”¨ MNRL å¾®è°ƒï¼Œå¯¹æ¯”å¾®è°ƒå‰åæ•ˆæœã€‚
Part B: Augmented SBERT â€” å…ˆè®­ç»ƒ cross-encoder æ ‡æ³¨ silver æ•°æ®ï¼Œ
         å†ç”¨ gold+silver è®­ç»ƒ bi-encoderï¼Œå¯¹æ¯” gold-only çš„æ•ˆæœã€‚
"""
import gc
import time
import numpy as np
import pandas as pd
import torch
from tqdm import tqdm
from datasets import Dataset, load_dataset
from sentence_transformers import (
    InputExample, SentenceTransformer, losses, models,
)
from sentence_transformers.cross_encoder import CrossEncoder
from sentence_transformers.datasets import NoDuplicatesDataLoader
from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator
from sentence_transformers.trainer import SentenceTransformerTrainer
from sentence_transformers.training_args import SentenceTransformerTrainingArguments

# ============================================================
# å…±ç”¨è¯„ä¼°å™¨ â€” STS-B
# ============================================================
total_start = time.time()

val_sts = load_dataset('glue', 'stsb', split='validation')
evaluator = EmbeddingSimilarityEvaluator(
    sentences1=val_sts["sentence1"],
    sentences2=val_sts["sentence2"],
    scores=[score/5 for score in val_sts["label"]],
    main_similarity="cosine"
)

# ================================================================
# Part A: å¾®è°ƒé¢„è®­ç»ƒ Sentence-Transformer (all-MiniLM-L6-v2)
# ================================================================
print("=" * 60)
print("Part A: å¾®è°ƒ all-MiniLM-L6-v2 (MNRL)")
print("=" * 60)
part_a_start = time.time()

# --- æ•°æ®å‡†å¤‡ (ä¸ 10.3 ç›¸åŒçš„ä¸‰å…ƒç»„æ ¼å¼) ---
mnli = load_dataset("glue", "mnli", split="train").select(range(50_000))
mnli = mnli.remove_columns("idx")
mapping = {2: 0, 1: 0, 0: 1}

# ä¸º Part B å‡†å¤‡ gold æ•°æ® (å‰ 10000 æ¡)
gold_dataset = load_dataset("glue", "mnli", split="train").select(range(10_000))
gold = pd.DataFrame({
    'sentence1': gold_dataset['premise'],
    'sentence2': gold_dataset['hypothesis'],
    'label': [mapping[label] for label in gold_dataset['label']]
})

# Part A æ•°æ®: entailment ä¸‰å…ƒç»„
import random
mnli_entailment = mnli.filter(lambda x: True if x['label'] == 0 else False)
train_data_a = {"anchor": [], "positive": [], "negative": []}
# ğŸ”§ ä¿®å¤ï¼šå…ˆè½¬æ¢ä¸º Python åˆ—è¡¨ï¼Œå†æ‰“ä¹±
soft_negatives = list(mnli_entailment["hypothesis"])  # è½¬æ¢ä¸º Python åˆ—è¡¨
random.shuffle(soft_negatives)                        # ç°åœ¨å¯ä»¥å®‰å…¨æ‰“ä¹±
for row, soft_negative in zip(mnli_entailment, soft_negatives):
    train_data_a["anchor"].append(row["premise"])
    train_data_a["positive"].append(row["hypothesis"])
    train_data_a["negative"].append(soft_negative)
train_dataset_a = Dataset.from_dict(train_data_a)

# --- å¾®è°ƒ ---
embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
train_loss = losses.MultipleNegativesRankingLoss(model=embedding_model)

args = SentenceTransformerTrainingArguments(
    output_dir="finetuned_embedding_model",
    num_train_epochs=1,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    warmup_steps=100,
    fp16=False,  # MPS (Apple Silicon) ä¸æ”¯æŒ fp16
    eval_steps=100,
    logging_steps=100,
    report_to="none",
)

trainer = SentenceTransformerTrainer(
    model=embedding_model,
    args=args,
    train_dataset=train_dataset_a,
    loss=train_loss,
    evaluator=evaluator
)
trainer.train()

print("\nå¾®è°ƒå STS-B è¯„ä¼°:")
results = evaluator(embedding_model)
for k, v in results.items():
    print(f"  {k}: {v:.4f}")

# --- å¯¹æ¯”: åŸå§‹é¢„è®­ç»ƒæ¨¡å‹ ---
print("\nåŸå§‹ all-MiniLM-L6-v2 STS-B è¯„ä¼°:")
original_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
results_orig = evaluator(original_model)
for k, v in results_orig.items():
    print(f"  {k}: {v:.4f}")

del embedding_model, original_model, trainer
gc.collect()
if torch.cuda.is_available():
    torch.cuda.empty_cache()
elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
    torch.mps.empty_cache()

# ================================================================
# Part B: Augmented SBERT
# ================================================================
print("\n" + "=" * 60)
print("Part B: Augmented SBERT")
print("=" * 60)
part_b_start = time.time()

# --- Step 1: è®­ç»ƒ Cross-Encoder ---
print("\nStep 1: è®­ç»ƒ Cross-Encoder (bert-base-uncased, 10k gold)")
gold_examples = [
    InputExample(texts=[row["premise"], row["hypothesis"]], label=mapping[row["label"]])
    for row in tqdm(gold_dataset)
]
gold_dataloader = NoDuplicatesDataLoader(gold_examples, batch_size=32)

cross_encoder = CrossEncoder('bert-base-uncased', num_labels=2)
cross_encoder.fit(
    train_dataloader=gold_dataloader,
    epochs=1,
    show_progress_bar=True,
    warmup_steps=100,
    use_amp=False
)

# --- Step 2: åˆ›å»ºæ–°çš„å¥å¯¹ ---
print("\nStep 2: å‡†å¤‡ silver å¥å¯¹ (10k~50k)")
silver_raw = load_dataset("glue", "mnli", split="train").select(range(10_000, 50_000))
pairs = list(zip(silver_raw['premise'], silver_raw['hypothesis']))

# --- Step 3: ç”¨ Cross-Encoder æ‰“æ ‡ (silver dataset) ---
print("\nStep 3: Cross-Encoder æ‰“æ ‡ silver æ•°æ®")
output = cross_encoder.predict(pairs, apply_softmax=True, show_progress_bar=True)
silver = pd.DataFrame({
    "sentence1": silver_raw["premise"],
    "sentence2": silver_raw["hypothesis"],
    "label": np.argmax(output, axis=1)
})
print(f"Silver æ•°æ®: {len(silver)} æ¡")
print(f"  æ ‡æ³¨ä¸º entailment: {(silver['label'] == 1).sum()}")
print(f"  æ ‡æ³¨ä¸º non-entailment: {(silver['label'] == 0).sum()}")

# --- Step 4: Gold + Silver è®­ç»ƒ Bi-Encoder ---
print("\nStep 4: Gold + Silver è®­ç»ƒ Bi-Encoder (CosineSimilarityLoss)")
data = pd.concat([gold, silver], ignore_index=True, axis=0)
data = data.drop_duplicates(subset=['sentence1', 'sentence2'], keep="first")
train_dataset_b = Dataset.from_pandas(data, preserve_index=False)
print(f"Gold+Silver åˆå¹¶å: {len(train_dataset_b)} æ¡")

embedding_model = SentenceTransformer('bert-base-uncased')
train_loss = losses.CosineSimilarityLoss(model=embedding_model)

args = SentenceTransformerTrainingArguments(
    output_dir="augmented_embedding_model",
    num_train_epochs=1,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    warmup_steps=100,
    fp16=False,  # MPS (Apple Silicon) ä¸æ”¯æŒ fp16
    eval_steps=100,
    logging_steps=100,
    report_to="none",
)

trainer = SentenceTransformerTrainer(
    model=embedding_model,
    args=args,
    train_dataset=train_dataset_b,
    loss=train_loss,
    evaluator=evaluator
)
trainer.train()

print("\nGold+Silver STS-B è¯„ä¼°:")
results_aug = evaluator(embedding_model)
for k, v in results_aug.items():
    print(f"  {k}: {v:.4f}")

trainer.accelerator.clear()
del embedding_model, trainer

# --- Step 5: ä»… Gold è®­ç»ƒå¯¹æ¯” ---
print("\nStep 5: ä»… Gold è®­ç»ƒ (å¯¹æ¯”)")
data_gold_only = gold.drop_duplicates(subset=['sentence1', 'sentence2'], keep="first")
train_dataset_gold = Dataset.from_pandas(data_gold_only, preserve_index=False)
print(f"Gold-only: {len(train_dataset_gold)} æ¡")

embedding_model = SentenceTransformer('bert-base-uncased')
train_loss = losses.CosineSimilarityLoss(model=embedding_model)

args = SentenceTransformerTrainingArguments(
    output_dir="gold_only_embedding_model",
    num_train_epochs=1,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    warmup_steps=100,
    fp16=False,  # MPS (Apple Silicon) ä¸æ”¯æŒ fp16
    eval_steps=100,
    logging_steps=100,
    report_to="none",
)

trainer = SentenceTransformerTrainer(
    model=embedding_model,
    args=args,
    train_dataset=train_dataset_gold,
    loss=train_loss,
    evaluator=evaluator
)
trainer.train()

print("\nGold-only STS-B è¯„ä¼°:")
results_gold = evaluator(embedding_model)
for k, v in results_gold.items():
    print(f"  {k}: {v:.4f}")

print("\nç»“è®º: ç›¸æ¯”ä»…ç”¨ gold æ•°æ®ï¼ŒåŠ å…¥ silver æ•°æ® (Augmented SBERT) å¯ä»¥æå‡æ¨¡å‹æ€§èƒ½!")
part_b_elapsed = time.time() - part_b_start
print(f"\nPart B è€—æ—¶: {part_b_elapsed:.1f}s ({part_b_elapsed/60:.1f}min)")

# ============================================================
# æ¸…ç†æ˜¾å­˜
# ============================================================
total_elapsed = time.time() - total_start
print(f"\næ€»è¿è¡Œæ—¶é—´: {total_elapsed:.1f}s ({total_elapsed/60:.1f}min)")

gc.collect()
if torch.cuda.is_available():
    torch.cuda.empty_cache()
elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
    torch.mps.empty_cache()
print("\næ˜¾å­˜å·²æ¸…ç†")
