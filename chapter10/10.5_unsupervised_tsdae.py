"""10.5 æ— ç›‘ç£å­¦ä¹  â€” TSDAE (Transformer-based Denoising AutoEncoder)
æ— éœ€ä»»ä½•æ ‡ç­¾ï¼Œé€šè¿‡ç»™è¾“å…¥æ–‡æœ¬æ·»åŠ å™ªå£° (éšæœºåˆ é™¤ token)ï¼Œ
è®©æ¨¡å‹å­¦ä¹ ä»æŸåçš„æ–‡æœ¬é‡å»ºåŸæ–‡ï¼Œä»è€Œå­¦åˆ°é«˜è´¨é‡çš„å¥å­åµŒå…¥ã€‚
"""
import gc
import time
import torch
import nltk
from tqdm import tqdm
from datasets import Dataset, load_dataset
from sentence_transformers import SentenceTransformer, losses, models
from sentence_transformers.datasets import DenoisingAutoEncoderDataset
from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator
from sentence_transformers.trainer import SentenceTransformerTrainer
from sentence_transformers.training_args import SentenceTransformerTrainingArguments

# ============================================================
# 0. ä¸‹è½½ NLTK åˆ†è¯å™¨
# ============================================================
nltk.download('punkt', quiet=True)
nltk.download('punkt_tab', quiet=True)

# ============================================================
# 1. æ•°æ®å‡†å¤‡ â€” æ„é€ å»å™ªæ•°æ®
# ============================================================
total_start = time.time()

print("=" * 60)
print("1. æ„é€  TSDAE å»å™ªè®­ç»ƒæ•°æ®")
print("=" * 60)

# Create a flat list of sentences (premise + hypothesis)
mnli = load_dataset("glue", "mnli", split="train").select(range(25_000))
# ğŸ”§ ä¿®å¤ï¼šå°† Column å¯¹è±¡è½¬æ¢ä¸ºåˆ—è¡¨ååˆå¹¶
flat_sentences = list(mnli["premise"]) + list(mnli["hypothesis"])
print(f"åŸå§‹å¥å­æ•°: {len(flat_sentences)}")

# Add noise to our input data (é»˜è®¤ del_ratio=0.6)
flat_sentences_unique = list(set(flat_sentences))
damaged_data = DenoisingAutoEncoderDataset(flat_sentences_unique)
print(f"å»é‡åå¥å­æ•°: {len(flat_sentences_unique)}")

# Create dataset
train_dataset = {"damaged_sentence": [], "original_sentence": []}
for data in tqdm(damaged_data, desc="æ„é€ å»å™ªæ•°æ®"):
    train_dataset["damaged_sentence"].append(data.texts[0])
    train_dataset["original_sentence"].append(data.texts[1])
train_dataset = Dataset.from_dict(train_dataset)

print(f"è®­ç»ƒé›†å¤§å°: {len(train_dataset)}")
print(f"\nç¤ºä¾‹:")
print(f"  æŸå: {train_dataset[0]['damaged_sentence'][:80]}...")
print(f"  åŸæ–‡: {train_dataset[0]['original_sentence'][:80]}...")

# ============================================================
# 2. è¯„ä¼°å™¨ â€” STS-B
# ============================================================
print("\n" + "=" * 60)
print("2. åˆ›å»º STS-B è¯„ä¼°å™¨")
print("=" * 60)

val_sts = load_dataset('glue', 'stsb', split='validation')
evaluator = EmbeddingSimilarityEvaluator(
    sentences1=val_sts["sentence1"],
    sentences2=val_sts["sentence2"],
    scores=[score/5 for score in val_sts["label"]],
    main_similarity="cosine"
)

# ============================================================
# 3. æ¨¡å‹ â€” æ‰‹åŠ¨æ„å»º (Transformer + CLS Pooling)
# ============================================================
print("\n" + "=" * 60)
print("3. æ„å»ºæ¨¡å‹ (bert-base-uncased + CLS pooling)")
print("=" * 60)

# TSDAE é€šå¸¸ä½¿ç”¨ CLS pooling è€Œé mean pooling
word_embedding_model = models.Transformer('bert-base-uncased')
pooling_model = models.Pooling(
    word_embedding_model.get_word_embedding_dimension(), 'cls'
)
embedding_model = SentenceTransformer(
    modules=[word_embedding_model, pooling_model]
)
print(f"Embedding ç»´åº¦: {word_embedding_model.get_word_embedding_dimension()}")

# ============================================================
# 4. æŸå¤±å‡½æ•° â€” DenoisingAutoEncoderLoss
# ============================================================
print("\n" + "=" * 60)
print("4. å®šä¹‰ DenoisingAutoEncoderLoss")
print("=" * 60)

train_loss = losses.DenoisingAutoEncoderLoss(
    embedding_model, tie_encoder_decoder=True
)

# å°† decoder ç§»åˆ°æ­£ç¡®çš„è®¾å¤‡ä¸Š
device = "cuda" if torch.cuda.is_available() else "mps" if (hasattr(torch.backends, "mps") and torch.backends.mps.is_available()) else "cpu"
train_loss.decoder = train_loss.decoder.to(device)
print(f"Decoder è®¾å¤‡: {device}")

# ============================================================
# 5. è®­ç»ƒ
# ============================================================
print("\n" + "=" * 60)
print("5. å¼€å§‹ TSDAE è®­ç»ƒ (1 epoch)")
print("=" * 60)

train_start = time.time()  # ğŸ”§ æ·»åŠ ç¼ºå¤±çš„è®¡æ—¶å˜é‡

args = SentenceTransformerTrainingArguments(
    output_dir="tsdae_embedding_model",
    num_train_epochs=1,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    warmup_steps=100,
    fp16=False,  # MPS (Apple Silicon) ä¸æ”¯æŒ fp16
    eval_steps=100,
    logging_steps=100,
    report_to="none",
)

trainer = SentenceTransformerTrainer(
    model=embedding_model,
    args=args,
    train_dataset=train_dataset,
    loss=train_loss,
    evaluator=evaluator
)
trainer.train()
train_elapsed = time.time() - train_start
print(f"\nè®­ç»ƒè€—æ—¶: {train_elapsed:.1f}s ({train_elapsed/60:.1f}min)")

# ============================================================
# 6. è¯„ä¼°
# ============================================================
print("\n" + "=" * 60)
print("6. STS-B è¯„ä¼°ç»“æœ")
print("=" * 60)

results = evaluator(embedding_model)
for k, v in results.items():
    print(f"  {k}: {v:.4f}")

print("\næ³¨æ„: TSDAE æ˜¯æ— ç›‘ç£æ–¹æ³•ï¼Œä¸éœ€è¦ä»»ä½•æ ‡æ³¨æ•°æ®!")
print("é€‚ç”¨åœºæ™¯: é¢†åŸŸé€‚åº” (domain adaptation)ã€å†·å¯åŠ¨æ—¶é¢„è®­ç»ƒ")

# ============================================================
# æ¸…ç†æ˜¾å­˜
# ============================================================
total_elapsed = time.time() - total_start
print(f"\næ€»è¿è¡Œæ—¶é—´: {total_elapsed:.1f}s ({total_elapsed/60:.1f}min)")

gc.collect()
if torch.cuda.is_available():
    torch.cuda.empty_cache()
elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
    torch.mps.empty_cache()
print("\næ˜¾å­˜å·²æ¸…ç†")
