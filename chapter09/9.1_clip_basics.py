import os
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

"""
9.1 CLIP åŸºç¡€ - å›¾æ–‡åµŒå…¥å¯¹é½
=================================

æœ¬èŠ‚å†…å®¹:
- CLIP æ¨¡å‹æ¶æ„ç†è§£
- å›¾åƒå’Œæ–‡æœ¬åµŒå…¥ç”Ÿæˆ
- ç»Ÿä¸€åµŒå…¥ç©ºé—´çš„æ¦‚å¿µ
- ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—

CLIP (Contrastive Language-Image Pre-training) æ˜¯ OpenAI å¼€å‘çš„å¤šæ¨¡æ€æ¨¡å‹ï¼Œ
é€šè¿‡å¯¹æ¯”å­¦ä¹ è®©å›¾åƒå’Œæ–‡æœ¬å…±äº«åŒä¸€ä¸ªåµŒå…¥ç©ºé—´ã€‚
"""

import warnings
warnings.filterwarnings("ignore")

import torch
import numpy as np
from PIL import Image
from urllib.request import urlopen


def get_device():
    """
    è‡ªåŠ¨æ£€æµ‹æœ€ä½³å¯ç”¨è®¾å¤‡
    ä¼˜å…ˆçº§: CUDA > MPS (Apple Silicon) > CPU
    """
    if torch.cuda.is_available():
        device = "cuda"
        device_name = torch.cuda.get_device_name(0)
        print(f"ä½¿ç”¨è®¾å¤‡: CUDA ({device_name})")
    elif torch.backends.mps.is_available():
        device = "mps"
        print("ä½¿ç”¨è®¾å¤‡: MPS (Apple Silicon GPU)")
    else:
        device = "cpu"
        print("ä½¿ç”¨è®¾å¤‡: CPU")
    return device


# ç¤ºä¾‹å›¾ç‰‡ URL
IMAGE_URLS = {
    "puppy": "https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-Language-Models/main/chapter09/images/puppy.png",
    "cat": "https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-Language-Models/main/chapter09/images/cat.png",
    "car": "https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-Language-Models/main/chapter09/images/car.png",
}


def load_image_from_url(url):
    """ä» URL åŠ è½½å›¾ç‰‡"""
    return Image.open(urlopen(url)).convert("RGB")


def clip_architecture_overview():
    """CLIP æ¶æ„æ¦‚è§ˆ"""
    print("=" * 60)
    print("CLIP æ¶æ„æ¦‚è§ˆ")
    print("=" * 60)
    
    architecture = """
CLIP åŒå¡”æ¶æ„:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     å›¾åƒ        â”‚         â”‚     æ–‡æœ¬        â”‚
â”‚   "puppy.jpg"   â”‚         â”‚   "a puppy"     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                           â”‚
         â–¼                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Vision Encoder â”‚         â”‚  Text Encoder   â”‚
â”‚   (ViT-B/32)    â”‚         â”‚ (Transformer)   â”‚
â”‚                 â”‚         â”‚                 â”‚
â”‚ â€¢ å›¾åƒåˆ†å—      â”‚         â”‚ â€¢ æ–‡æœ¬åˆ†è¯      â”‚
â”‚ â€¢ PatchåµŒå…¥     â”‚         â”‚ â€¢ ä½ç½®ç¼–ç       â”‚
â”‚ â€¢ Transformer   â”‚         â”‚ â€¢ è‡ªæ³¨æ„åŠ›      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                           â”‚
         â–¼                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   å›¾åƒåµŒå…¥      â”‚â—„â”€â”€ç›¸ä¼¼åº¦â”€â”€â–ºâ”‚   æ–‡æœ¬åµŒå…¥      â”‚
â”‚    [512ç»´]      â”‚   è®¡ç®—    â”‚    [512ç»´]      â”‚
â”‚                 â”‚         â”‚                 â”‚
â”‚ â€¢ L2 å½’ä¸€åŒ–     â”‚         â”‚ â€¢ L2 å½’ä¸€åŒ–     â”‚
â”‚ â€¢ ä½™å¼¦ç›¸ä¼¼åº¦    â”‚         â”‚ â€¢ ä½™å¼¦ç›¸ä¼¼åº¦    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

å…³é”®ç‰¹ç‚¹:
â€¢ ç»Ÿä¸€åµŒå…¥ç©ºé—´: å›¾åƒå’Œæ–‡æœ¬æ˜ å°„åˆ°ç›¸åŒçš„512ç»´ç©ºé—´
â€¢ å¯¹æ¯”å­¦ä¹ : åŒ¹é…çš„å›¾æ–‡å¯¹ç›¸ä¼¼åº¦é«˜ï¼Œä¸åŒ¹é…çš„ç›¸ä¼¼åº¦ä½
â€¢ é›¶æ ·æœ¬èƒ½åŠ›: æ— éœ€é¢å¤–è®­ç»ƒå³å¯è¿›è¡Œå›¾æ–‡åŒ¹é…
â€¢ é¢„è®­ç»ƒæ•°æ®: 4äº¿ä¸ªå›¾æ–‡å¯¹ (WITæ•°æ®é›†)
"""
    print(architecture)


def clip_embeddings_demo(device=None):
    """
    CLIP å›¾æ–‡åµŒå…¥æ¼”ç¤º
    å±•ç¤ºå¦‚ä½•ä½¿ç”¨ CLIP ç”Ÿæˆå›¾åƒå’Œæ–‡æœ¬çš„ç»Ÿä¸€åµŒå…¥
    """
    from transformers import CLIPTokenizerFast, CLIPProcessor, CLIPModel
    
    print("\n" + "=" * 60)
    print("9.1 CLIP åŸºç¡€ - å›¾æ–‡åµŒå…¥å¯¹é½")
    print("=" * 60)
    
    if device is None:
        device = get_device()
    
    # 1. æ¨¡å‹åŠ è½½
    print("\n[æ­¥éª¤ 1] åŠ è½½ CLIP æ¨¡å‹...")
    model_id = "openai/clip-vit-base-patch32"
    
    # åˆ†åˆ«åŠ è½½å„ä¸ªç»„ä»¶
    clip_tokenizer = CLIPTokenizerFast.from_pretrained(model_id)
    clip_processor = CLIPProcessor.from_pretrained(model_id)
    model = CLIPModel.from_pretrained(model_id).to(device)
    
    print(f"âœ“ æ¨¡å‹: {model_id}")
    print(f"âœ“ æ–‡æœ¬åµŒå…¥ç»´åº¦: 512")
    print(f"âœ“ å›¾åƒåµŒå…¥ç»´åº¦: 512 (ç»Ÿä¸€ç©ºé—´)")
    print(f"âœ“ å‚æ•°é‡: ~151M")
    
    # 2. åŠ è½½ç¤ºä¾‹æ•°æ®
    print("\n[æ­¥éª¤ 2] åŠ è½½ç¤ºä¾‹å›¾åƒå’Œæ–‡æœ¬...")
    image = load_image_from_url(IMAGE_URLS["puppy"])
    caption = "a puppy playing in the snow"
    
    print(f"âœ“ å›¾åƒ: é›ªåœ°å°ç‹—")
    print(f"âœ“ æè¿°: '{caption}'")
    print(f"âœ“ å›¾åƒå°ºå¯¸: {image.size}")
    
    # 3. æ–‡æœ¬å¤„ç†å’ŒåµŒå…¥
    print("\n[æ­¥éª¤ 3] ç”Ÿæˆæ–‡æœ¬åµŒå…¥...")
    text_inputs = clip_tokenizer(caption, return_tensors="pt").to(device)
    
    # å±•ç¤ºåˆ†è¯ç»“æœ
    tokens = clip_tokenizer.convert_ids_to_tokens(text_inputs["input_ids"][0])
    print(f"âœ“ åˆ†è¯ç»“æœ: {tokens}")
    print(f"âœ“ Tokenæ•°é‡: {len(tokens)}")
    
    with torch.no_grad():
        text_embedding = model.get_text_features(**text_inputs)
    
    print(f"âœ“ æ–‡æœ¬åµŒå…¥å½¢çŠ¶: {text_embedding.shape}")
    print(f"âœ“ åµŒå…¥èŒƒå›´: [{text_embedding.min():.3f}, {text_embedding.max():.3f}]")
    
    # 4. å›¾åƒå¤„ç†å’ŒåµŒå…¥
    print("\n[æ­¥éª¤ 4] ç”Ÿæˆå›¾åƒåµŒå…¥...")
    image_inputs = clip_processor(images=image, return_tensors="pt").to(device)
    
    print(f"âœ“ å›¾åƒå¼ é‡å½¢çŠ¶: {image_inputs['pixel_values'].shape}")
    print("  â†’ [batch_size, channels, height, width] = [1, 3, 224, 224]")
    print(f"âœ“ åƒç´ å€¼èŒƒå›´: [{image_inputs['pixel_values'].min():.3f}, {image_inputs['pixel_values'].max():.3f}]")
    
    with torch.no_grad():
        image_embedding = model.get_image_features(**image_inputs)
    
    print(f"âœ“ å›¾åƒåµŒå…¥å½¢çŠ¶: {image_embedding.shape}")
    print(f"âœ“ åµŒå…¥èŒƒå›´: [{image_embedding.min():.3f}, {image_embedding.max():.3f}]")
    
    # 5. ç›¸ä¼¼åº¦è®¡ç®—
    print("\n[æ­¥éª¤ 5] è®¡ç®—å›¾æ–‡ç›¸ä¼¼åº¦...")
    
    # L2 å½’ä¸€åŒ– (CLIP çš„æ ‡å‡†åšæ³•)
    text_embedding_norm = text_embedding / text_embedding.norm(dim=-1, keepdim=True)
    image_embedding_norm = image_embedding / image_embedding.norm(dim=-1, keepdim=True)
    
    # ä½™å¼¦ç›¸ä¼¼åº¦ (å½’ä¸€åŒ–åçš„ç‚¹ç§¯)
    similarity = (text_embedding_norm @ image_embedding_norm.T).item()
    
    print(f"âœ“ åŸå§‹åµŒå…¥æ¨¡é•¿:")
    print(f"  - æ–‡æœ¬: {text_embedding.norm():.3f}")
    print(f"  - å›¾åƒ: {image_embedding.norm():.3f}")
    print(f"âœ“ å½’ä¸€åŒ–åæ¨¡é•¿: 1.000 (æ ‡å‡†åŒ–)")
    print(f"âœ“ ä½™å¼¦ç›¸ä¼¼åº¦: {similarity:.4f}")
    
    # 6. ç›¸ä¼¼åº¦è§£é‡Š
    print("\n[æ­¥éª¤ 6] ç›¸ä¼¼åº¦è§£é‡Š...")
    if similarity > 0.3:
        print(f"ğŸ¯ é«˜ç›¸ä¼¼åº¦ ({similarity:.4f}) - å›¾æ–‡åŒ¹é…è‰¯å¥½!")
    elif similarity > 0.1:
        print(f"ğŸ” ä¸­ç­‰ç›¸ä¼¼åº¦ ({similarity:.4f}) - å›¾æ–‡æœ‰ä¸€å®šå…³è”")
    else:
        print(f"âŒ ä½ç›¸ä¼¼åº¦ ({similarity:.4f}) - å›¾æ–‡ä¸åŒ¹é…")
    
    print("\nç›¸ä¼¼åº¦èŒƒå›´è¯´æ˜:")
    print("â€¢ [0.8, 1.0]: å®Œç¾åŒ¹é…")
    print("â€¢ [0.5, 0.8]: å¼ºç›¸å…³")
    print("â€¢ [0.2, 0.5]: ä¸­ç­‰ç›¸å…³")
    print("â€¢ [0.0, 0.2]: å¼±ç›¸å…³")
    print("â€¢ [-1.0, 0.0]: è´Ÿç›¸å…³")
    
    return model, clip_processor, clip_tokenizer


def demonstrate_embedding_properties(model, clip_processor, clip_tokenizer, device):
    """æ¼”ç¤ºåµŒå…¥çš„æ€§è´¨"""
    print("\n" + "=" * 60)
    print("åµŒå…¥ç©ºé—´æ€§è´¨æ¼”ç¤º")
    print("=" * 60)
    
    # æµ‹è¯•å¤šä¸ªæ–‡æœ¬æè¿°
    texts = [
        "a puppy playing in the snow",
        "a dog in winter",
        "a cat sleeping",
        "a car on the road",
        "snow and animals"
    ]
    
    # åŠ è½½å›¾åƒ
    image = load_image_from_url(IMAGE_URLS["puppy"])
    
    print("\nè®¡ç®—å›¾åƒä¸å¤šä¸ªæ–‡æœ¬çš„ç›¸ä¼¼åº¦...")
    
    # ç”Ÿæˆå›¾åƒåµŒå…¥
    image_inputs = clip_processor(images=image, return_tensors="pt").to(device)
    with torch.no_grad():
        image_embedding = model.get_image_features(**image_inputs)
        image_embedding = image_embedding / image_embedding.norm(dim=-1, keepdim=True)
    
    # è®¡ç®—ä¸æ¯ä¸ªæ–‡æœ¬çš„ç›¸ä¼¼åº¦
    similarities = []
    for text in texts:
        text_inputs = clip_tokenizer(text, return_tensors="pt").to(device)
        with torch.no_grad():
            text_embedding = model.get_text_features(**text_inputs)
            text_embedding = text_embedding / text_embedding.norm(dim=-1, keepdim=True)
        
        sim = (image_embedding @ text_embedding.T).item()
        similarities.append(sim)
        print(f"'{text}': {sim:.4f}")
    
    # æ‰¾åˆ°æœ€ä½³åŒ¹é…
    best_idx = np.argmax(similarities)
    print(f"\nğŸ† æœ€ä½³åŒ¹é…: '{texts[best_idx]}' (ç›¸ä¼¼åº¦: {similarities[best_idx]:.4f})")
    
    return similarities


def technical_details():
    """CLIP æŠ€æœ¯ç»†èŠ‚"""
    print("\n" + "=" * 60)
    print("CLIP æŠ€æœ¯ç»†èŠ‚")
    print("=" * 60)
    
    details = """
1. æ¨¡å‹æ¶æ„:
   â€¢ Vision Encoder: Vision Transformer (ViT-B/32)
     - è¾“å…¥: 224Ã—224 RGBå›¾åƒ
     - Patchå¤§å°: 32Ã—32 (196ä¸ªpatches)
     - å±‚æ•°: 12å±‚ Transformer
     - æ³¨æ„åŠ›å¤´: 12ä¸ª
     - éšè—ç»´åº¦: 768
   
   â€¢ Text Encoder: Transformer
     - æœ€å¤§åºåˆ—é•¿åº¦: 77 tokens
     - å±‚æ•°: 12å±‚
     - æ³¨æ„åŠ›å¤´: 8ä¸ª
     - éšè—ç»´åº¦: 512
   
   â€¢ æŠ•å½±å±‚: å°†ä¸¤ä¸ªç¼–ç å™¨è¾“å‡ºæŠ•å½±åˆ°512ç»´ç©ºé—´

2. è®­ç»ƒè¿‡ç¨‹:
   â€¢ æ•°æ®: 4äº¿ä¸ªå›¾æ–‡å¯¹ (ä»äº’è”ç½‘æ”¶é›†)
   â€¢ æŸå¤±å‡½æ•°: å¯¹æ¯”æŸå¤± (Contrastive Loss)
   â€¢ æ‰¹æ¬¡å¤§å°: 32,768
   â€¢ è®­ç»ƒæ—¶é—´: 12å¤© (592ä¸ªV100 GPU)
   
3. å¯¹æ¯”å­¦ä¹ åŸç†:
   â€¢ æ­£æ ·æœ¬: åŒ¹é…çš„å›¾æ–‡å¯¹ï¼Œç›¸ä¼¼åº¦æœ€å¤§åŒ–
   â€¢ è´Ÿæ ·æœ¬: ä¸åŒ¹é…çš„å›¾æ–‡å¯¹ï¼Œç›¸ä¼¼åº¦æœ€å°åŒ–
   â€¢ æ¸©åº¦å‚æ•°: æ§åˆ¶åˆ†å¸ƒçš„é”åº¦
   
4. é›¶æ ·æœ¬èƒ½åŠ›:
   â€¢ å›¾åƒåˆ†ç±»: å°†ç±»åˆ«åè½¬æ¢ä¸ºæ–‡æœ¬ï¼Œè®¡ç®—ç›¸ä¼¼åº¦
   â€¢ å›¾åƒæ£€ç´¢: ç”¨æ–‡æœ¬æŸ¥è¯¢æ‰¾åˆ°æœ€ç›¸ä¼¼çš„å›¾åƒ
   â€¢ æ–‡æœ¬æ£€ç´¢: ç”¨å›¾åƒæŸ¥è¯¢æ‰¾åˆ°æœ€ç›¸ä¼¼çš„æ–‡æœ¬

5. ä¼˜åŠ¿:
   âœ“ æ— éœ€æ ‡æ³¨æ•°æ®è¿›è¡Œåˆ†ç±»
   âœ“ æ³›åŒ–èƒ½åŠ›å¼º
   âœ“ å¤šè¯­è¨€æ”¯æŒ
   âœ“ é²æ£’æ€§å¥½
   
6. å±€é™æ€§:
   âœ— ç»†ç²’åº¦ç†è§£æœ‰é™
   âœ— å¤æ‚æ¨ç†èƒ½åŠ›å¼±
   âœ— ç”Ÿæˆèƒ½åŠ›ç¼ºå¤±
   âœ— å¯¹æŠ½è±¡æ¦‚å¿µç†è§£ä¸è¶³
"""
    print(details)


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹ CLIP åŸºç¡€å­¦ä¹ ...")
    
    # æ¶æ„æ¦‚è§ˆ
    clip_architecture_overview()
    
    # è®¾å¤‡æ£€æµ‹
    device = get_device()
    
    try:
        # åŸºç¡€æ¼”ç¤º
        model, processor, tokenizer = clip_embeddings_demo(device)
        
        # åµŒå…¥æ€§è´¨æ¼”ç¤º
        demonstrate_embedding_properties(model, processor, tokenizer, device)
        
        # æŠ€æœ¯ç»†èŠ‚
        technical_details()
        
        print("\n" + "=" * 60)
        print("âœ… 9.1 CLIP åŸºç¡€å­¦ä¹ å®Œæˆ!")
        print("=" * 60)
        print("\nä¸‹ä¸€æ­¥: è¿è¡Œ 9.2_clip_similarity_matrix.py å­¦ä¹ ç›¸ä¼¼åº¦çŸ©é˜µ")
        
    except Exception as e:
        print(f"\nâŒ è¿è¡Œå‡ºé”™: {e}")
        print("è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œä¾èµ–å®‰è£…")


if __name__ == "__main__":
    main()