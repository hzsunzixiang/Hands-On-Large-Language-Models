import os
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

"""
9.5 è½»é‡çº§è§†è§‰è¯­è¨€æ¨¡å‹
======================

æœ¬èŠ‚å†…å®¹:
- è½»é‡çº§ VLM æ¨¡å‹é€‰æ‹©
- BLIP-base å›¾åƒæè¿°
- èµ„æºå‹å¥½çš„éƒ¨ç½²æ–¹æ¡ˆ
- è¾¹ç¼˜è®¾å¤‡é€‚é…
- æ€§èƒ½ä¸èµ„æºçš„æƒè¡¡

å½“ BLIP-2 ç­‰å¤§æ¨¡å‹èµ„æºéœ€æ±‚è¿‡é«˜æ—¶ï¼Œè½»é‡çº§æ¨¡å‹æä¾›äº†
å®ç”¨çš„æ›¿ä»£æ–¹æ¡ˆï¼Œé€‚åˆèµ„æºå—é™çš„ç¯å¢ƒã€‚
"""

import warnings
warnings.filterwarnings("ignore")

import torch
import numpy as np
from PIL import Image
from urllib.request import urlopen
import time
import gc


def get_device():
    """è‡ªåŠ¨æ£€æµ‹æœ€ä½³å¯ç”¨è®¾å¤‡"""
    if torch.cuda.is_available():
        device = "cuda"
        device_name = torch.cuda.get_device_name(0)
        memory_gb = torch.cuda.get_device_properties(0).total_memory / 1e9
        print(f"ä½¿ç”¨è®¾å¤‡: CUDA ({device_name}, {memory_gb:.1f}GB)")
    elif torch.backends.mps.is_available():
        device = "mps"
        print("ä½¿ç”¨è®¾å¤‡: MPS (Apple Silicon GPU)")
    else:
        device = "cpu"
        print("ä½¿ç”¨è®¾å¤‡: CPU")
    return device


# ç¤ºä¾‹å›¾ç‰‡
IMAGE_URLS = {
    "puppy": "https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-Language-Models/main/chapter09/images/puppy.png",
    "beach": "https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-Language-Models/main/chapter09/images/beach.png",
    "car": "https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-Language-Models/main/chapter09/images/car.png",
}


def load_image_from_url(url):
    """ä» URL åŠ è½½å›¾ç‰‡"""
    return Image.open(urlopen(url)).convert("RGB")


def lightweight_vlm_overview():
    """è½»é‡çº§ VLM æ¦‚è§ˆ"""
    print("=" * 60)
    print("è½»é‡çº§è§†è§‰è¯­è¨€æ¨¡å‹æ¦‚è§ˆ")
    print("=" * 60)
    
    overview = """
è½»é‡çº§ VLM çš„å¿…è¦æ€§:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

1. èµ„æºé™åˆ¶åœºæ™¯:
   â€¢ è¾¹ç¼˜è®¾å¤‡ (æ‰‹æœºã€åµŒå…¥å¼ç³»ç»Ÿ)
   â€¢ äº‘æœåŠ¡æˆæœ¬æ§åˆ¶
   â€¢ å®æ—¶åº”ç”¨éœ€æ±‚
   â€¢ ç¦»çº¿éƒ¨ç½²éœ€æ±‚

2. å¤§æ¨¡å‹çš„æŒ‘æˆ˜:
   â€¢ BLIP-2: ~15GB å†…å­˜éœ€æ±‚
   â€¢ æ¨ç†é€Ÿåº¦æ…¢
   â€¢ éƒ¨ç½²æˆæœ¬é«˜
   â€¢ èƒ½è€—å¤§

è½»é‡çº§æ¨¡å‹å¯¹æ¯”:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     æ¨¡å‹        â”‚   å¤§å°   â”‚   å†…å­˜   â”‚   é€Ÿåº¦   â”‚   èƒ½åŠ›   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ BLIP-2-OPT-2.7B â”‚  ~15GB   â”‚  ~15GB   â”‚    æ…¢    â”‚   å¼ºå¤§   â”‚
â”‚ BLIP-base       â”‚  ~1GB    â”‚  ~2GB    â”‚    å¿«    â”‚   ä¸­ç­‰   â”‚
â”‚ CLIP            â”‚  ~600MB  â”‚  ~1GB    â”‚   å¾ˆå¿«   â”‚  ä»…åµŒå…¥  â”‚
â”‚ MiniGPT-4       â”‚  ~7GB    â”‚  ~8GB    â”‚   ä¸­ç­‰   â”‚   è¾ƒå¼º   â”‚
â”‚ LLaVA-7B        â”‚  ~13GB   â”‚  ~14GB   â”‚   ä¸­ç­‰   â”‚   å¼ºå¤§   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

è½»é‡çº§æ¨¡å‹çš„ä¼˜åŠ¿:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âœ“ å¿«é€Ÿéƒ¨ç½²: æ¨¡å‹ä¸‹è½½å’ŒåŠ è½½é€Ÿåº¦å¿«
âœ“ ä½å»¶è¿Ÿ: æ¨ç†é€Ÿåº¦å¿«ï¼Œé€‚åˆå®æ—¶åº”ç”¨
âœ“ ä½æˆæœ¬: ç¡¬ä»¶è¦æ±‚ä½ï¼Œè¿è¡Œæˆæœ¬ä½
âœ“ æ˜“é›†æˆ: ç®€å•çš„ APIï¼Œæ˜“äºé›†æˆåˆ°åº”ç”¨ä¸­
âœ“ ç¦»çº¿å‹å¥½: å¯åœ¨æ— ç½‘ç»œç¯å¢ƒä¸‹è¿è¡Œ

åº”ç”¨åœºæ™¯:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

1. ç§»åŠ¨åº”ç”¨:
   â€¢ å›¾ç‰‡è‡ªåŠ¨æ ‡æ³¨
   â€¢ å®æ—¶å›¾åƒæè¿°
   â€¢ è¾…åŠ©åŠŸèƒ½ (è§†è§‰è¾…åŠ©)

2. è¾¹ç¼˜è®¡ç®—:
   â€¢ IoT è®¾å¤‡å›¾åƒåˆ†æ
   â€¢ ç›‘æ§ç³»ç»Ÿ
   â€¢ è‡ªåŠ¨é©¾é©¶è¾…åŠ©

3. åŸå‹å¼€å‘:
   â€¢ å¿«é€Ÿæ¦‚å¿µéªŒè¯
   â€¢ æ•™å­¦æ¼”ç¤º
   â€¢ ç®—æ³•ç ”ç©¶

4. æ‰¹é‡å¤„ç†:
   â€¢ å¤§è§„æ¨¡å›¾åƒæ ‡æ³¨
   â€¢ å†…å®¹å®¡æ ¸
   â€¢ æ•°æ®é¢„å¤„ç†
"""
    print(overview)


def model_comparison_analysis():
    """æ¨¡å‹å¯¹æ¯”åˆ†æ"""
    print("\n" + "=" * 60)
    print("æ¨¡å‹è¯¦ç»†å¯¹æ¯”åˆ†æ")
    print("=" * 60)
    
    analysis = """
BLIP-base vs BLIP-2 è¯¦ç»†å¯¹æ¯”:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

BLIP-base (Salesforce/blip-image-captioning-base):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ æ¶æ„: Vision Transformer + BERT-base                       â”‚
â”‚ å‚æ•°é‡: ~385M                                               â”‚
â”‚ å†…å­˜éœ€æ±‚: ~2GB                                              â”‚
â”‚ æ¨ç†é€Ÿåº¦: å¿« (~100ms/image)                                 â”‚
â”‚ æ”¯æŒä»»åŠ¡:                                                   â”‚
â”‚   âœ“ æ— æ¡ä»¶å›¾åƒæè¿°                                          â”‚
â”‚   âœ“ æ¡ä»¶å›¾åƒæè¿° (å¸¦å‰ç¼€)                                   â”‚
â”‚   âœ— å¤æ‚è§†è§‰é—®ç­”                                            â”‚
â”‚   âœ— å¤šè½®å¯¹è¯                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

BLIP-2-OPT-2.7B:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ æ¶æ„: ViT + Q-Former + OPT-2.7B                           â”‚
â”‚ å‚æ•°é‡: ~15B                                                â”‚
â”‚ å†…å­˜éœ€æ±‚: ~15GB                                             â”‚
â”‚ æ¨ç†é€Ÿåº¦: æ…¢ (~2s/image)                                    â”‚
â”‚ æ”¯æŒä»»åŠ¡:                                                   â”‚
â”‚   âœ“ å¤æ‚å›¾åƒæè¿°                                            â”‚
â”‚   âœ“ è§†è§‰é—®ç­”                                                â”‚
â”‚   âœ“ å¤šè½®å¯¹è¯                                                â”‚
â”‚   âœ“ æŒ‡ä»¤è·Ÿéš                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

é€‰æ‹©å»ºè®®:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ä½¿ç”¨ BLIP-base çš„åœºæ™¯:
â€¢ åªéœ€è¦åŸºç¡€å›¾åƒæè¿°
â€¢ èµ„æºå—é™ (< 4GB GPUå†…å­˜)
â€¢ éœ€è¦å¿«é€Ÿå“åº” (< 200ms)
â€¢ æ‰¹é‡å¤„ç†å¤§é‡å›¾åƒ
â€¢ ç§»åŠ¨ç«¯/è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²

ä½¿ç”¨ BLIP-2 çš„åœºæ™¯:
â€¢ éœ€è¦å¤æ‚è§†è§‰ç†è§£
â€¢ æ”¯æŒè‡ªç„¶è¯­è¨€é—®ç­”
â€¢ å¤šè½®å¯¹è¯éœ€æ±‚
â€¢ æœ‰å……è¶³è®¡ç®—èµ„æº (> 16GB GPU)
â€¢ å¯¹å‡†ç¡®æ€§è¦æ±‚é«˜äºé€Ÿåº¦

æ··åˆç­–ç•¥:
â€¢ ç”¨ CLIP åšåˆæ­¥ç­›é€‰
â€¢ ç”¨ BLIP-base åšåŸºç¡€æè¿°
â€¢ ç”¨ BLIP-2 åšå¤æ‚åˆ†æ
"""
    print(analysis)


def blip_base_demo(device):
    """BLIP-base æ¼”ç¤º"""
    print("\n" + "=" * 60)
    print("BLIP-base è½»é‡çº§å›¾åƒæè¿°")
    print("=" * 60)
    
    from transformers import BlipProcessor, BlipForConditionalGeneration
    
    # 1. åŠ è½½æ¨¡å‹
    print("\n[æ­¥éª¤ 1] åŠ è½½ BLIP-base æ¨¡å‹...")
    model_id = "Salesforce/blip-image-captioning-base"
    
    try:
        start_time = time.time()
        
        processor = BlipProcessor.from_pretrained(model_id)
        model = BlipForConditionalGeneration.from_pretrained(model_id)
        
        if device != "cpu":
            model = model.to(device)
        
        load_time = time.time() - start_time
        
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        print(f"â±ï¸  åŠ è½½æ—¶é—´: {load_time:.1f}s")
        
        # æ¨¡å‹ä¿¡æ¯
        total_params = sum(p.numel() for p in model.parameters())
        print(f"ğŸ“Š å‚æ•°é‡: {total_params/1e6:.1f}M")
        
        if device == "cuda":
            memory_mb = torch.cuda.memory_allocated() / 1e6
            print(f"ğŸ’¾ GPUå†…å­˜ä½¿ç”¨: {memory_mb:.0f}MB")
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None, None
    
    # 2. æ— æ¡ä»¶å›¾åƒæè¿°
    print(f"\n[æ­¥éª¤ 2] æ— æ¡ä»¶å›¾åƒæè¿°...")
    
    for name, url in IMAGE_URLS.items():
        try:
            print(f"\nğŸ–¼ï¸  æµ‹è¯•å›¾åƒ: {name}")
            
            # åŠ è½½å›¾åƒ
            image = load_image_from_url(url)
            print(f"   å›¾åƒå°ºå¯¸: {image.size}")
            
            # é¢„å¤„ç† (æ— æ–‡æœ¬è¾“å…¥)
            inputs = processor(image, return_tensors="pt")
            if device != "cpu":
                inputs = inputs.to(device)
            
            # ç”Ÿæˆæè¿°
            start_time = time.time()
            
            with torch.no_grad():
                out = model.generate(
                    **inputs,
                    max_length=50,
                    num_beams=3,
                    temperature=0.7,
                    do_sample=True
                )
            
            generation_time = time.time() - start_time
            
            # è§£ç 
            caption = processor.decode(out[0], skip_special_tokens=True)
            
            print(f"   ğŸ¤– æè¿°: {caption}")
            print(f"   â±ï¸  ç”Ÿæˆæ—¶é—´: {generation_time:.3f}s")
            
        except Exception as e:
            print(f"   âŒ å¤„ç†å¤±è´¥: {e}")
    
    # 3. æ¡ä»¶å›¾åƒæè¿°
    print(f"\n[æ­¥éª¤ 3] æ¡ä»¶å›¾åƒæè¿°...")
    
    conditional_tasks = [
        {
            "image_key": "puppy",
            "prefixes": [
                "a photo of",
                "this image shows",
                "in this picture"
            ]
        },
        {
            "image_key": "beach", 
            "prefixes": [
                "a beautiful",
                "this landscape shows",
                "the scene depicts"
            ]
        }
    ]
    
    for task in conditional_tasks:
        image_key = task["image_key"]
        prefixes = task["prefixes"]
        
        print(f"\nğŸ–¼ï¸  å›¾åƒ: {image_key}")
        
        try:
            image = load_image_from_url(IMAGE_URLS[image_key])
            
            for prefix in prefixes:
                print(f"\n   å‰ç¼€: '{prefix}'")
                
                # å¸¦æ–‡æœ¬å‰ç¼€çš„é¢„å¤„ç†
                inputs = processor(image, text=prefix, return_tensors="pt")
                if device != "cpu":
                    inputs = inputs.to(device)
                
                # ç”Ÿæˆ
                with torch.no_grad():
                    out = model.generate(
                        **inputs,
                        max_length=50,
                        num_beams=2,
                        temperature=0.6
                    )
                
                # è§£ç 
                caption = processor.decode(out[0], skip_special_tokens=True)
                
                print(f"   ğŸ¤– å®Œæ•´æè¿°: {caption}")
                
        except Exception as e:
            print(f"   âŒ å¤„ç†å¤±è´¥: {e}")
    
    return model, processor


def performance_benchmark(model, processor, device):
    """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    print("\n" + "=" * 60)
    print("æ€§èƒ½åŸºå‡†æµ‹è¯•")
    print("=" * 60)
    
    # æµ‹è¯•é…ç½®
    test_configs = [
        {"batch_size": 1, "max_length": 20, "num_beams": 1},
        {"batch_size": 1, "max_length": 30, "num_beams": 2},
        {"batch_size": 1, "max_length": 50, "num_beams": 3},
    ]
    
    test_image = load_image_from_url(IMAGE_URLS["car"])
    
    print("ğŸ”¬ ä¸åŒé…ç½®çš„æ€§èƒ½æµ‹è¯•:")
    print("-" * 50)
    
    for i, config in enumerate(test_configs):
        batch_size = config["batch_size"]
        max_length = config["max_length"]
        num_beams = config["num_beams"]
        
        print(f"\n[æµ‹è¯• {i+1}] max_length={max_length}, num_beams={num_beams}")
        
        # å¤šæ¬¡æµ‹è¯•å–å¹³å‡
        times = []
        captions = []
        
        for _ in range(3):  # 3æ¬¡æµ‹è¯•
            inputs = processor(test_image, return_tensors="pt")
            if device != "cpu":
                inputs = inputs.to(device)
            
            start_time = time.time()
            
            with torch.no_grad():
                out = model.generate(
                    **inputs,
                    max_length=max_length,
                    num_beams=num_beams,
                    pad_token_id=processor.tokenizer.eos_token_id
                )
            
            end_time = time.time()
            
            caption = processor.decode(out[0], skip_special_tokens=True)
            
            times.append(end_time - start_time)
            captions.append(caption)
        
        # ç»Ÿè®¡ç»“æœ
        avg_time = np.mean(times)
        std_time = np.std(times)
        
        print(f"   å¹³å‡æ—¶é—´: {avg_time:.3f}s (Â±{std_time:.3f}s)")
        print(f"   ç¤ºä¾‹è¾“å‡º: {captions[0]}")
        
        # è®¡ç®—ååé‡
        throughput = 1 / avg_time
        print(f"   ååé‡: {throughput:.1f} images/s")
    
    # å†…å­˜ä½¿ç”¨ç»Ÿè®¡
    if device == "cuda":
        print(f"\nğŸ’¾ å†…å­˜ä½¿ç”¨ç»Ÿè®¡:")
        memory_allocated = torch.cuda.memory_allocated() / 1e6
        memory_reserved = torch.cuda.memory_reserved() / 1e6
        print(f"   å·²åˆ†é…: {memory_allocated:.0f}MB")
        print(f"   å·²ä¿ç•™: {memory_reserved:.0f}MB")


def batch_processing_demo(model, processor, device):
    """æ‰¹é‡å¤„ç†æ¼”ç¤º"""
    print("\n" + "=" * 60)
    print("æ‰¹é‡å¤„ç†æ¼”ç¤º")
    print("=" * 60)
    
    # å‡†å¤‡å¤šå¼ å›¾åƒ
    images = []
    image_names = []
    
    for name, url in IMAGE_URLS.items():
        try:
            img = load_image_from_url(url)
            images.append(img)
            image_names.append(name)
        except:
            pass
    
    print(f"ğŸ“¦ æ‰¹é‡å¤„ç† {len(images)} å¼ å›¾åƒ...")
    
    # æ–¹æ³•1: é€ä¸ªå¤„ç†
    print(f"\n[æ–¹æ³• 1] é€ä¸ªå¤„ç†:")
    
    start_time = time.time()
    individual_captions = []
    
    for i, (image, name) in enumerate(zip(images, image_names)):
        inputs = processor(image, return_tensors="pt")
        if device != "cpu":
            inputs = inputs.to(device)
        
        with torch.no_grad():
            out = model.generate(**inputs, max_length=30, num_beams=2)
        
        caption = processor.decode(out[0], skip_special_tokens=True)
        individual_captions.append(caption)
        
        print(f"   {name}: {caption}")
    
    individual_time = time.time() - start_time
    print(f"   æ€»æ—¶é—´: {individual_time:.2f}s")
    print(f"   å¹³å‡: {individual_time/len(images):.2f}s/image")
    
    # æ–¹æ³•2: æ‰¹é‡å¤„ç† (å¦‚æœæ”¯æŒ)
    print(f"\n[æ–¹æ³• 2] æ‰¹é‡å¤„ç†:")
    
    try:
        start_time = time.time()
        
        # æ³¨æ„: BLIP å¯èƒ½ä¸æ”¯æŒçœŸæ­£çš„æ‰¹é‡å¤„ç†ï¼Œè¿™é‡Œæ¼”ç¤ºæ¦‚å¿µ
        batch_captions = []
        
        # æ¨¡æ‹Ÿæ‰¹é‡å¤„ç† (å®é™…ä¸Šè¿˜æ˜¯é€ä¸ªï¼Œä½†å¯ä»¥ä¼˜åŒ–é¢„å¤„ç†)
        for image, name in zip(images, image_names):
            inputs = processor(image, return_tensors="pt")
            if device != "cpu":
                inputs = inputs.to(device)
            
            with torch.no_grad():
                out = model.generate(**inputs, max_length=30, num_beams=2)
            
            caption = processor.decode(out[0], skip_special_tokens=True)
            batch_captions.append(caption)
        
        batch_time = time.time() - start_time
        
        print(f"   æ‰¹é‡æ—¶é—´: {batch_time:.2f}s")
        print(f"   åŠ é€Ÿæ¯”: {individual_time/batch_time:.1f}x")
        
    except Exception as e:
        print(f"   æ‰¹é‡å¤„ç†ä¸æ”¯æŒ: {e}")


def deployment_considerations():
    """éƒ¨ç½²è€ƒè™‘å› ç´ """
    print("\n" + "=" * 60)
    print("éƒ¨ç½²è€ƒè™‘å› ç´ ")
    print("=" * 60)
    
    considerations = """
1. ç¡¬ä»¶è¦æ±‚:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

æœ€ä½é…ç½®:
â€¢ CPU: 4æ ¸å¿ƒä»¥ä¸Š
â€¢ å†…å­˜: 4GB RAM
â€¢ å­˜å‚¨: 2GB å¯ç”¨ç©ºé—´
â€¢ GPU: å¯é€‰ (åŠ é€Ÿæ¨ç†)

æ¨èé…ç½®:
â€¢ CPU: 8æ ¸å¿ƒä»¥ä¸Š
â€¢ å†…å­˜: 8GB RAM  
â€¢ GPU: 4GB+ VRAM (GTX 1660 æˆ–æ›´å¥½)
â€¢ å­˜å‚¨: SSD å­˜å‚¨

2. è½¯ä»¶ç¯å¢ƒ:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ä¾èµ–é¡¹:
```bash
pip install torch torchvision
pip install transformers
pip install pillow
pip install numpy
```

Docker éƒ¨ç½²:
```dockerfile
FROM python:3.9-slim
RUN pip install torch transformers pillow
COPY app.py /app/
WORKDIR /app
CMD ["python", "app.py"]
```

3. æ€§èƒ½ä¼˜åŒ–:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

æ¨¡å‹ä¼˜åŒ–:
â€¢ ä½¿ç”¨ FP16 ç²¾åº¦ (å‡å°‘å†…å­˜ä½¿ç”¨)
â€¢ æ¨¡å‹é‡åŒ– (INT8)
â€¢ åŠ¨æ€æ‰¹å¤„ç†
â€¢ ç¼“å­˜æœºåˆ¶

ä»£ç ä¼˜åŒ–:
```python
# FP16 æ¨ç†
model = model.half()
inputs = {k: v.half() if v.dtype == torch.float32 else v 
          for k, v in inputs.items()}

# æ‰¹é‡é¢„å¤„ç†
def preprocess_batch(images):
    return processor(images, return_tensors="pt", padding=True)
```

4. æ‰©å±•æ€§è€ƒè™‘:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

æ°´å¹³æ‰©å±•:
â€¢ è´Ÿè½½å‡è¡¡
â€¢ å¤šå®ä¾‹éƒ¨ç½²
â€¢ é˜Ÿåˆ—ç³»ç»Ÿ (Redis/RabbitMQ)

å‚ç›´æ‰©å±•:
â€¢ GPU é›†ç¾¤
â€¢ æ¨¡å‹å¹¶è¡Œ
â€¢ æµæ°´çº¿å¹¶è¡Œ

5. ç›‘æ§å’Œç»´æŠ¤:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

å…³é”®æŒ‡æ ‡:
â€¢ æ¨ç†å»¶è¿Ÿ (P50, P95, P99)
â€¢ ååé‡ (QPS)
â€¢ å†…å­˜ä½¿ç”¨ç‡
â€¢ GPU åˆ©ç”¨ç‡
â€¢ é”™è¯¯ç‡

æ—¥å¿—è®°å½•:
â€¢ è¯·æ±‚/å“åº”æ—¥å¿—
â€¢ æ€§èƒ½æŒ‡æ ‡
â€¢ é”™è¯¯è¿½è¸ª
â€¢ èµ„æºä½¿ç”¨æƒ…å†µ

6. æˆæœ¬åˆ†æ:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

BLIP-base vs BLIP-2 æˆæœ¬å¯¹æ¯”:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    æŒ‡æ ‡     â”‚ BLIP-baseâ”‚  BLIP-2  â”‚   èŠ‚çœ   â”‚   è¯´æ˜   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPU å†…å­˜    â”‚   2GB    â”‚   15GB   â”‚   87%    â”‚ ç¡¬ä»¶æˆæœ¬ â”‚
â”‚ æ¨ç†å»¶è¿Ÿ    â”‚  100ms   â”‚   2000ms â”‚   95%    â”‚ ç”¨æˆ·ä½“éªŒ â”‚
â”‚ æœåŠ¡å™¨æˆæœ¬  â”‚  $50/æœˆ  â”‚ $400/æœˆ  â”‚   87%    â”‚ è¿è¥æˆæœ¬ â”‚
â”‚ ç”µåŠ›æ¶ˆè€—    â”‚   ä½     â”‚    é«˜    â”‚   80%    â”‚ ç¯ä¿è€ƒè™‘ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
"""
    print(considerations)


def edge_device_simulation():
    """è¾¹ç¼˜è®¾å¤‡æ¨¡æ‹Ÿ"""
    print("\n" + "=" * 60)
    print("è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²æ¨¡æ‹Ÿ")
    print("=" * 60)
    
    print("ğŸ”§ æ¨¡æ‹Ÿèµ„æºå—é™ç¯å¢ƒ...")
    
    # æ¨¡æ‹Ÿä¸åŒçš„èµ„æºé™åˆ¶
    scenarios = [
        {
            "name": "ç§»åŠ¨è®¾å¤‡",
            "cpu_cores": 4,
            "memory_gb": 2,
            "gpu": False,
            "description": "æ™ºèƒ½æ‰‹æœº/å¹³æ¿"
        },
        {
            "name": "è¾¹ç¼˜æœåŠ¡å™¨", 
            "cpu_cores": 8,
            "memory_gb": 4,
            "gpu": True,
            "description": "å°å‹è¾¹ç¼˜è®¡ç®—èŠ‚ç‚¹"
        },
        {
            "name": "åµŒå…¥å¼è®¾å¤‡",
            "cpu_cores": 2, 
            "memory_gb": 1,
            "gpu": False,
            "description": "Raspberry Pi ç­‰"
        }
    ]
    
    for scenario in scenarios:
        name = scenario["name"]
        cpu_cores = scenario["cpu_cores"]
        memory_gb = scenario["memory_gb"]
        has_gpu = scenario["gpu"]
        desc = scenario["description"]
        
        print(f"\nğŸ“± åœºæ™¯: {name} ({desc})")
        print(f"   CPU: {cpu_cores} æ ¸å¿ƒ")
        print(f"   å†…å­˜: {memory_gb}GB")
        print(f"   GPU: {'æ˜¯' if has_gpu else 'å¦'}")
        
        # è¯„ä¼°é€‚ç”¨æ€§
        if memory_gb >= 2 and cpu_cores >= 4:
            print("   âœ… é€‚åˆ BLIP-base")
            print("   âŒ ä¸é€‚åˆ BLIP-2")
            
            # ä¼°ç®—æ€§èƒ½
            if has_gpu:
                estimated_time = "100-200ms"
                throughput = "5-10 images/s"
            else:
                estimated_time = "500-1000ms"  
                throughput = "1-2 images/s"
                
            print(f"   â±ï¸  é¢„ä¼°å»¶è¿Ÿ: {estimated_time}")
            print(f"   ğŸ“Š é¢„ä¼°åå: {throughput}")
            
        elif memory_gb >= 1:
            print("   âš ï¸  ä»…é€‚åˆ CLIP (åµŒå…¥)")
            print("   âŒ ä¸é€‚åˆç”Ÿæˆæ¨¡å‹")
        else:
            print("   âŒ èµ„æºä¸è¶³")
    
    # ä¼˜åŒ–å»ºè®®
    print(f"\nğŸ’¡ è¾¹ç¼˜éƒ¨ç½²ä¼˜åŒ–å»ºè®®:")
    print("   1. ä½¿ç”¨æ¨¡å‹é‡åŒ– (INT8)")
    print("   2. å¯ç”¨æ¨¡å‹ç¼“å­˜")
    print("   3. æ‰¹é‡å¤„ç†ä¼˜åŒ–")
    print("   4. å¼‚æ­¥æ¨ç†é˜Ÿåˆ—")
    print("   5. ç»“æœç¼“å­˜æœºåˆ¶")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹è½»é‡çº§ VLM å­¦ä¹ ...")
    
    # æ¦‚è§ˆ
    lightweight_vlm_overview()
    
    # æ¨¡å‹å¯¹æ¯”
    model_comparison_analysis()
    
    # è®¾å¤‡æ£€æµ‹
    device = get_device()
    
    try:
        # BLIP-base æ¼”ç¤º
        model, processor = blip_base_demo(device)
        
        if model is not None and processor is not None:
            # æ€§èƒ½æµ‹è¯•
            performance_benchmark(model, processor, device)
            
            # æ‰¹é‡å¤„ç†
            batch_processing_demo(model, processor, device)
        
        # éƒ¨ç½²è€ƒè™‘
        deployment_considerations()
        
        # è¾¹ç¼˜è®¾å¤‡æ¨¡æ‹Ÿ
        edge_device_simulation()
        
        print("\n" + "=" * 60)
        print("âœ… 9.5 è½»é‡çº§ VLM å­¦ä¹ å®Œæˆ!")
        print("=" * 60)
        print("\nğŸ¯ å…³é”®æ”¶è·:")
        print("  â€¢ è½»é‡çº§æ¨¡å‹çš„ä¼˜åŠ¿å’Œé€‚ç”¨åœºæ™¯")
        print("  â€¢ BLIP-base çš„å®é™…æ€§èƒ½è¡¨ç°")
        print("  â€¢ èµ„æºä¸æ€§èƒ½çš„æƒè¡¡è€ƒè™‘")
        print("  â€¢ è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²çš„å®è·µæŒ‡å¯¼")
        print("\nä¸‹ä¸€æ­¥: è¿è¡Œ 9.6_multimodal_summary.py æŸ¥çœ‹ç« èŠ‚æ€»ç»“")
        
    except Exception as e:
        print(f"\nâŒ è¿è¡Œå‡ºé”™: {e}")
        print("è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œä¾èµ–å®‰è£…")
    
    finally:
        # æ¸…ç†å†…å­˜
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()


if __name__ == "__main__":
    main()