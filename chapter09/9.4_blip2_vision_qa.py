import os
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

"""
9.4 BLIP-2 è§†è§‰é—®ç­”ç³»ç»Ÿ
========================

æœ¬èŠ‚å†…å®¹:
- BLIP-2 æ¶æ„æ·±å…¥ç†è§£
- å›¾åƒæè¿°ç”Ÿæˆ (Image Captioning)
- è§†è§‰é—®ç­” (Visual Question Answering)
- å¤šè½®å¯¹è¯å¼è§†è§‰é—®ç­”
- æ¨¡å‹ä¼˜åŒ–å’Œéƒ¨ç½²è€ƒè™‘

BLIP-2 æ˜¯ Salesforce å¼€å‘çš„å…ˆè¿›è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œ
é€šè¿‡ Q-Former æ¶æ„å®ç°äº†å¼ºå¤§çš„å›¾æ–‡ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚
"""

import warnings
warnings.filterwarnings("ignore")

import torch
import numpy as np
from PIL import Image
from urllib.request import urlopen
import gc


def get_device():
    """è‡ªåŠ¨æ£€æµ‹æœ€ä½³å¯ç”¨è®¾å¤‡"""
    if torch.cuda.is_available():
        device = "cuda"
        device_name = torch.cuda.get_device_name(0)
        memory_gb = torch.cuda.get_device_properties(0).total_memory / 1e9
        print(f"ä½¿ç”¨è®¾å¤‡: CUDA ({device_name})")
        print(f"GPU å†…å­˜: {memory_gb:.1f} GB")
    elif torch.backends.mps.is_available():
        device = "mps"
        print("ä½¿ç”¨è®¾å¤‡: MPS (Apple Silicon GPU)")
    else:
        device = "cpu"
        print("ä½¿ç”¨è®¾å¤‡: CPU")
    return device


# ç¤ºä¾‹å›¾ç‰‡
IMAGE_URLS = {
    "puppy": "https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-Language-Models/main/chapter09/images/puppy.png",
    "beach": "https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-Language-Models/main/chapter09/images/beach.png",
    "car": "https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-Language-Models/main/chapter09/images/car.png",
}


def load_image_from_url(url):
    """ä» URL åŠ è½½å›¾ç‰‡"""
    return Image.open(urlopen(url)).convert("RGB")


def blip2_architecture_overview():
    """BLIP-2 æ¶æ„è¯¦è§£"""
    print("=" * 60)
    print("BLIP-2 æ¶æ„è¯¦è§£")
    print("=" * 60)
    
    architecture = """
BLIP-2 ä¸‰é˜¶æ®µæ¶æ„è®¾è®¡:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        è¾“å…¥å›¾åƒ                              â”‚
â”‚                     (224Ã—224Ã—3)                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Vision Encoder                              â”‚
â”‚                   (å†»ç»“å‚æ•°)                                 â”‚
â”‚  â€¢ ViT-L/14 æˆ– ViT-g/14                                    â”‚
â”‚  â€¢ è¾“å‡º: å›¾åƒç‰¹å¾ [batch, 257, 1408]                        â”‚
â”‚  â€¢ é¢„è®­ç»ƒæƒé‡ä¿æŒä¸å˜                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ å›¾åƒç‰¹å¾
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Q-Former                                  â”‚
â”‚                 (å¯å­¦ä¹ å‚æ•°)                                 â”‚
â”‚  â€¢ 32 ä¸ªå¯å­¦ä¹ æŸ¥è¯¢å‘é‡ (Learnable Queries)                   â”‚
â”‚  â€¢ 12 å±‚ Transformer                                       â”‚
â”‚  â€¢ è‡ªæ³¨æ„åŠ› + äº¤å‰æ³¨æ„åŠ›                                     â”‚
â”‚  â€¢ è¾“å‡º: è§†è§‰ tokens [batch, 32, 768]                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ è§†è§‰ tokens
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                å¤§è¯­è¨€æ¨¡å‹ (LLM)                              â”‚
â”‚                   (å†»ç»“å‚æ•°)                                 â”‚
â”‚  â€¢ OPT-2.7B / FlanT5-XL                                   â”‚
â”‚  â€¢ æ¥æ”¶è§†è§‰ tokens ä½œä¸ºå‰ç¼€                                 â”‚
â”‚  â€¢ ç”Ÿæˆæ–‡æœ¬å›ç­”                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æ ¸å¿ƒåˆ›æ–° - Q-Former:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

1. å¯å­¦ä¹ æŸ¥è¯¢ (Learnable Queries):
   â€¢ 32 ä¸ªå›ºå®šæ•°é‡çš„æŸ¥è¯¢å‘é‡
   â€¢ é€šè¿‡äº¤å‰æ³¨æ„åŠ›ä»å›¾åƒä¸­æå–ä¿¡æ¯
   â€¢ å‹ç¼©å›¾åƒä¿¡æ¯ä¸ºå›ºå®šé•¿åº¦è¡¨ç¤º

2. ä¸‰ç§æ³¨æ„åŠ›æœºåˆ¶:
   â€¢ è‡ªæ³¨æ„åŠ›: æŸ¥è¯¢ä¹‹é—´çš„äº¤äº’
   â€¢ äº¤å‰æ³¨æ„åŠ›: æŸ¥è¯¢ä¸å›¾åƒç‰¹å¾çš„äº¤äº’  
   â€¢ å› æœæ³¨æ„åŠ›: æ–‡æœ¬ç”Ÿæˆæ—¶çš„æ©ç æ³¨æ„åŠ›

3. è®­ç»ƒç­–ç•¥:
   â€¢ é˜¶æ®µ1: å›¾æ–‡å¯¹æ¯”å­¦ä¹  + å›¾æ–‡åŒ¹é… + å›¾åƒæè¿°ç”Ÿæˆ
   â€¢ é˜¶æ®µ2: æŒ‡ä»¤å¾®è°ƒï¼Œå¯¹é½è§†è§‰å’Œè¯­è¨€ç†è§£

ä¼˜åŠ¿:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âœ“ æ¨¡å—åŒ–è®¾è®¡: å„ç»„ä»¶å¯ç‹¬ç«‹ä¼˜åŒ–
âœ“ å‚æ•°æ•ˆç‡: åªè®­ç»ƒ Q-Formerï¼Œå…¶ä»–ç»„ä»¶å†»ç»“
âœ“ å¼ºå¤§ç”Ÿæˆ: åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›
âœ“ å¤šä»»åŠ¡æ”¯æŒ: å›¾åƒæè¿°ã€è§†è§‰é—®ç­”ã€å¯¹è¯

æŠ€æœ¯è§„æ ¼:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

â€¢ æ€»å‚æ•°é‡: ~15B (OPT-2.7B ç‰ˆæœ¬)
â€¢ å¯è®­ç»ƒå‚æ•°: ~188M (ä»… Q-Former)
â€¢ å†…å­˜éœ€æ±‚: ~15GB GPU å†…å­˜
â€¢ æ¨ç†é€Ÿåº¦: ä¸­ç­‰ (å— LLM å¤§å°å½±å“)
"""
    print(architecture)


def check_system_requirements():
    """æ£€æŸ¥ç³»ç»Ÿè¦æ±‚"""
    print("\n" + "=" * 60)
    print("ç³»ç»Ÿè¦æ±‚æ£€æŸ¥")
    print("=" * 60)
    
    device = get_device()
    
    # æ£€æŸ¥ GPU å†…å­˜
    if device == "cuda":
        memory_gb = torch.cuda.get_device_properties(0).total_memory / 1e9
        print(f"\nğŸ’¾ GPU å†…å­˜æ£€æŸ¥:")
        print(f"  å¯ç”¨å†…å­˜: {memory_gb:.1f} GB")
        
        if memory_gb >= 16:
            print("  âœ… å†…å­˜å……è¶³ï¼Œå¯è¿è¡Œ BLIP-2")
        elif memory_gb >= 8:
            print("  âš ï¸  å†…å­˜è¾ƒå°‘ï¼Œå»ºè®®ä½¿ç”¨ FP16 ç²¾åº¦")
        else:
            print("  âŒ å†…å­˜ä¸è¶³ï¼Œå»ºè®®ä½¿ç”¨è¾ƒå°æ¨¡å‹")
    
    elif device == "mps":
        print(f"\nğŸ’¾ MPS è®¾å¤‡:")
        print("  âš ï¸  Apple Silicon GPUï¼Œå†…å­˜å…±äº«")
        print("  å»ºè®®ç›‘æ§å†…å­˜ä½¿ç”¨æƒ…å†µ")
    
    else:
        print(f"\nğŸ’¾ CPU æ¨¡å¼:")
        print("  âš ï¸  æ¨ç†é€Ÿåº¦è¾ƒæ…¢")
        print("  å»ºè®®ä½¿ç”¨è¾ƒå°çš„æ¨¡å‹")
    
    # æ£€æŸ¥ä¾èµ–
    print(f"\nğŸ“¦ ä¾èµ–æ£€æŸ¥:")
    try:
        from transformers import Blip2Processor, Blip2ForConditionalGeneration
        print("  âœ… transformers åº“å·²å®‰è£…")
    except ImportError:
        print("  âŒ éœ€è¦å®‰è£…: pip install transformers")
        return False
    
    try:
        import torch
        print(f"  âœ… PyTorch {torch.__version__}")
    except ImportError:
        print("  âŒ éœ€è¦å®‰è£… PyTorch")
        return False
    
    return True


def load_blip2_model(device, model_size="2.7b"):
    """åŠ è½½ BLIP-2 æ¨¡å‹"""
    print(f"\n[æ¨¡å‹åŠ è½½] åŠ è½½ BLIP-2 æ¨¡å‹...")
    
    from transformers import Blip2Processor, Blip2ForConditionalGeneration
    
    # æ¨¡å‹é€‰æ‹©
    model_configs = {
        "2.7b": "Salesforce/blip2-opt-2.7b",
        "6.7b": "Salesforce/blip2-opt-6.7b", 
        "flan-t5-xl": "Salesforce/blip2-flan-t5-xl"
    }
    
    model_id = model_configs.get(model_size, model_configs["2.7b"])
    print(f"âœ“ é€‰æ‹©æ¨¡å‹: {model_id}")
    
    try:
        # åŠ è½½å¤„ç†å™¨
        processor = Blip2Processor.from_pretrained(model_id)
        print("âœ“ å¤„ç†å™¨åŠ è½½å®Œæˆ")
        
        # åŠ è½½æ¨¡å‹ (ä¼˜åŒ–å†…å­˜ä½¿ç”¨)
        print("  æ­£åœ¨åŠ è½½æ¨¡å‹æƒé‡...")
        
        model_kwargs = {}
        
        if device == "cuda":
            model_kwargs.update({
                "torch_dtype": torch.float16,
                "device_map": "auto",
                "load_in_8bit": False  # å¯è®¾ä¸º True è¿›ä¸€æ­¥èŠ‚çœå†…å­˜
            })
        elif device == "mps":
            model_kwargs.update({
                "torch_dtype": torch.float16
            })
        else:
            model_kwargs.update({
                "torch_dtype": torch.float32
            })
        
        model = Blip2ForConditionalGeneration.from_pretrained(
            model_id,
            **model_kwargs
        )
        
        if device == "mps":
            model = model.to(device)
        
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        print(f"ğŸ“Š æ¨¡å‹ä¿¡æ¯:")
        print(f"  æ€»å‚æ•°é‡: {total_params/1e9:.1f}B")
        print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params/1e6:.1f}M")
        print(f"  å†»ç»“æ¯”ä¾‹: {(1-trainable_params/total_params)*100:.1f}%")
        
        return model, processor
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        print("\nğŸ’¡ å»ºè®®:")
        print("  1. æ£€æŸ¥ç½‘ç»œè¿æ¥")
        print("  2. ç¡®ä¿æœ‰è¶³å¤Ÿçš„å†…å­˜/å­˜å‚¨ç©ºé—´")
        print("  3. å°è¯•ä½¿ç”¨è¾ƒå°çš„æ¨¡å‹")
        return None, None


def image_captioning_demo(model, processor, device):
    """å›¾åƒæè¿°ç”Ÿæˆæ¼”ç¤º"""
    print("\n" + "=" * 60)
    print("å›¾åƒæè¿°ç”Ÿæˆæ¼”ç¤º")
    print("=" * 60)
    
    # æµ‹è¯•ä¸åŒç±»å‹çš„å›¾åƒæè¿°
    captioning_tasks = [
        {
            "image_key": "puppy",
            "prompts": [
                "Question: What do you see in this image? Answer:",
                "Question: Describe this image in detail. Answer:",
                "Question: Write a caption for this photo. Answer:"
            ]
        },
        {
            "image_key": "beach", 
            "prompts": [
                "Question: What is the setting of this image? Answer:",
                "Question: Describe the landscape. Answer:"
            ]
        },
        {
            "image_key": "car",
            "prompts": [
                "Question: What vehicle is shown? Answer:",
                "Question: Describe the car and its surroundings. Answer:"
            ]
        }
    ]
    
    for task in captioning_tasks:
        image_key = task["image_key"]
        prompts = task["prompts"]
        
        print(f"\nğŸ–¼ï¸  æµ‹è¯•å›¾åƒ: {image_key}")
        print("-" * 40)
        
        try:
            # åŠ è½½å›¾åƒ
            image = load_image_from_url(IMAGE_URLS[image_key])
            print(f"âœ“ å›¾åƒå°ºå¯¸: {image.size}")
            
            for i, prompt in enumerate(prompts):
                print(f"\n[ä»»åŠ¡ {i+1}] {prompt}")
                
                # é¢„å¤„ç†
                inputs = processor(image, text=prompt, return_tensors="pt")
                
                # ç§»åŠ¨åˆ°è®¾å¤‡
                if device != "cpu":
                    inputs = {k: v.to(device) for k, v in inputs.items()}
                    if device == "cuda":
                        inputs = {k: v.half() if v.dtype == torch.float32 else v 
                                for k, v in inputs.items()}
                
                # ç”Ÿæˆ
                with torch.no_grad():
                    generated_ids = model.generate(
                        **inputs,
                        max_new_tokens=50,
                        num_beams=3,
                        temperature=0.7,
                        do_sample=True,
                        pad_token_id=processor.tokenizer.eos_token_id
                    )
                
                # è§£ç 
                generated_text = processor.batch_decode(
                    generated_ids, 
                    skip_special_tokens=True
                )[0].strip()
                
                # æ¸…ç†è¾“å‡º (ç§»é™¤æç¤ºéƒ¨åˆ†)
                if "Answer:" in generated_text:
                    answer = generated_text.split("Answer:")[-1].strip()
                else:
                    answer = generated_text
                
                print(f"ğŸ¤– å›ç­”: {answer}")
                
        except Exception as e:
            print(f"âŒ å¤„ç† {image_key} æ—¶å‡ºé”™: {e}")


def visual_question_answering_demo(model, processor, device):
    """è§†è§‰é—®ç­”æ¼”ç¤º"""
    print("\n" + "=" * 60)
    print("è§†è§‰é—®ç­”æ¼”ç¤º")
    print("=" * 60)
    
    # å®šä¹‰é—®ç­”ä»»åŠ¡
    vqa_tasks = [
        {
            "image_key": "puppy",
            "questions": [
                "What animal is in the image?",
                "What is the dog doing?",
                "What season does this appear to be?",
                "Is the dog indoors or outdoors?",
                "What color is the dog's fur?"
            ]
        },
        {
            "image_key": "beach",
            "questions": [
                "What type of landscape is this?",
                "Is this a natural or artificial environment?",
                "What time of day might this be?",
                "Are there any people visible?",
                "What's the weather like?"
            ]
        },
        {
            "image_key": "car",
            "questions": [
                "What type of vehicle is shown?",
                "What color is the car?",
                "Is the car moving or stationary?",
                "What kind of road is the car on?",
                "How many cars are visible?"
            ]
        }
    ]
    
    for task in vqa_tasks:
        image_key = task["image_key"]
        questions = task["questions"]
        
        print(f"\nğŸ–¼ï¸  å›¾åƒ: {image_key}")
        print("=" * 40)
        
        try:
            # åŠ è½½å›¾åƒ
            image = load_image_from_url(IMAGE_URLS[image_key])
            
            for i, question in enumerate(questions):
                prompt = f"Question: {question} Answer:"
                
                # é¢„å¤„ç†
                inputs = processor(image, text=prompt, return_tensors="pt")
                
                # ç§»åŠ¨åˆ°è®¾å¤‡
                if device != "cpu":
                    inputs = {k: v.to(device) for k, v in inputs.items()}
                    if device == "cuda":
                        inputs = {k: v.half() if v.dtype == torch.float32 else v 
                                for k, v in inputs.items()}
                
                # ç”Ÿæˆå›ç­”
                with torch.no_grad():
                    generated_ids = model.generate(
                        **inputs,
                        max_new_tokens=30,
                        num_beams=2,
                        temperature=0.5,
                        pad_token_id=processor.tokenizer.eos_token_id
                    )
                
                # è§£ç 
                generated_text = processor.batch_decode(
                    generated_ids,
                    skip_special_tokens=True
                )[0].strip()
                
                # æå–ç­”æ¡ˆ
                if "Answer:" in generated_text:
                    answer = generated_text.split("Answer:")[-1].strip()
                else:
                    answer = generated_text
                
                print(f"â“ Q{i+1}: {question}")
                print(f"ğŸ¤– A{i+1}: {answer}\n")
                
        except Exception as e:
            print(f"âŒ å¤„ç† {image_key} æ—¶å‡ºé”™: {e}")


def conversational_vqa_demo(model, processor, device):
    """å¤šè½®å¯¹è¯å¼è§†è§‰é—®ç­”"""
    print("\n" + "=" * 60)
    print("å¤šè½®å¯¹è¯å¼è§†è§‰é—®ç­”")
    print("=" * 60)
    
    # é€‰æ‹©ä¸€å¼ å›¾åƒè¿›è¡Œæ·±å…¥å¯¹è¯
    image_key = "puppy"
    image = load_image_from_url(IMAGE_URLS[image_key])
    
    print(f"ğŸ–¼ï¸  å¯¹è¯å›¾åƒ: {image_key}")
    print("ğŸ’¬ å¼€å§‹å¤šè½®å¯¹è¯...")
    
    # å®šä¹‰å¯¹è¯æµç¨‹
    conversation_flow = [
        "What do you see in this image?",
        "What is the dog doing specifically?", 
        "What might the weather be like?",
        "Is this a good environment for the dog?",
        "What breed might this dog be?",
        "Would you recommend any activities for this dog?"
    ]
    
    conversation_history = []
    
    for i, question in enumerate(conversation_flow):
        print(f"\n--- è½®æ¬¡ {i+1} ---")
        print(f"ğŸ‘¤ ç”¨æˆ·: {question}")
        
        # æ„å»ºå¸¦å†å²çš„æç¤º
        if conversation_history:
            # åŒ…å«ä¹‹å‰çš„å¯¹è¯å†å²
            history_text = " ".join([
                f"Q: {q} A: {a}" for q, a in conversation_history[-2:]  # ä¿ç•™æœ€è¿‘2è½®
            ])
            prompt = f"{history_text} Question: {question} Answer:"
        else:
            prompt = f"Question: {question} Answer:"
        
        try:
            # é¢„å¤„ç†
            inputs = processor(image, text=prompt, return_tensors="pt")
            
            # ç§»åŠ¨åˆ°è®¾å¤‡
            if device != "cpu":
                inputs = {k: v.to(device) for k, v in inputs.items()}
                if device == "cuda":
                    inputs = {k: v.half() if v.dtype == torch.float32 else v 
                            for k, v in inputs.items()}
            
            # ç”Ÿæˆå›ç­”
            with torch.no_grad():
                generated_ids = model.generate(
                    **inputs,
                    max_new_tokens=40,
                    num_beams=3,
                    temperature=0.6,
                    do_sample=True,
                    pad_token_id=processor.tokenizer.eos_token_id
                )
            
            # è§£ç 
            generated_text = processor.batch_decode(
                generated_ids,
                skip_special_tokens=True
            )[0].strip()
            
            # æå–ç­”æ¡ˆ
            if "Answer:" in generated_text:
                answer = generated_text.split("Answer:")[-1].strip()
            else:
                answer = generated_text
            
            print(f"ğŸ¤– BLIP-2: {answer}")
            
            # æ·»åŠ åˆ°å¯¹è¯å†å²
            conversation_history.append((question, answer))
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {e}")
            break
    
    print(f"\nâœ… å¯¹è¯å®Œæˆï¼Œå…± {len(conversation_history)} è½®")


def performance_analysis(model, processor, device):
    """æ€§èƒ½åˆ†æ"""
    print("\n" + "=" * 60)
    print("æ€§èƒ½åˆ†æ")
    print("=" * 60)
    
    import time
    
    # æµ‹è¯•å›¾åƒ
    image = load_image_from_url(IMAGE_URLS["car"])
    
    # æµ‹è¯•ä¸åŒé•¿åº¦çš„ç”Ÿæˆ
    test_configs = [
        {"max_tokens": 10, "description": "çŸ­å›ç­”"},
        {"max_tokens": 30, "description": "ä¸­ç­‰å›ç­”"},
        {"max_tokens": 50, "description": "é•¿å›ç­”"}
    ]
    
    prompt = "Question: Describe this image in detail. Answer:"
    
    print("ğŸ”¬ ç”Ÿæˆé•¿åº¦å¯¹æ€§èƒ½çš„å½±å“:")
    print("-" * 50)
    
    for config in test_configs:
        max_tokens = config["max_tokens"]
        desc = config["description"]
        
        # é¢„å¤„ç†
        inputs = processor(image, text=prompt, return_tensors="pt")
        
        if device != "cpu":
            inputs = {k: v.to(device) for k, v in inputs.items()}
            if device == "cuda":
                inputs = {k: v.half() if v.dtype == torch.float32 else v 
                        for k, v in inputs.items()}
        
        # æµ‹é‡æ—¶é—´
        start_time = time.time()
        
        with torch.no_grad():
            generated_ids = model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                num_beams=2,
                pad_token_id=processor.tokenizer.eos_token_id
            )
        
        end_time = time.time()
        
        # è§£ç 
        generated_text = processor.batch_decode(
            generated_ids,
            skip_special_tokens=True
        )[0].strip()
        
        if "Answer:" in generated_text:
            answer = generated_text.split("Answer:")[-1].strip()
        else:
            answer = generated_text
        
        # è®¡ç®—ç»Ÿè®¡
        generation_time = end_time - start_time
        tokens_generated = len(processor.tokenizer.encode(answer))
        tokens_per_second = tokens_generated / generation_time if generation_time > 0 else 0
        
        print(f"\n{desc} (max_tokens={max_tokens}):")
        print(f"  ç”Ÿæˆæ—¶é—´: {generation_time:.2f}s")
        print(f"  å®é™…tokens: {tokens_generated}")
        print(f"  ç”Ÿæˆé€Ÿåº¦: {tokens_per_second:.1f} tokens/s")
        print(f"  å›ç­”: {answer[:100]}...")
    
    # å†…å­˜ä½¿ç”¨åˆ†æ
    if device == "cuda":
        print(f"\nğŸ’¾ GPU å†…å­˜ä½¿ç”¨:")
        memory_allocated = torch.cuda.memory_allocated() / 1e9
        memory_reserved = torch.cuda.memory_reserved() / 1e9
        print(f"  å·²åˆ†é…: {memory_allocated:.1f} GB")
        print(f"  å·²ä¿ç•™: {memory_reserved:.1f} GB")


def cleanup_memory():
    """æ¸…ç†å†…å­˜"""
    print("\nğŸ§¹ æ¸…ç†å†…å­˜...")
    
    # æ¸…ç† Python åƒåœ¾å›æ”¶
    gc.collect()
    
    # æ¸…ç† CUDA ç¼“å­˜
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        print("âœ“ CUDA ç¼“å­˜å·²æ¸…ç†")
    
    print("âœ“ å†…å­˜æ¸…ç†å®Œæˆ")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹ BLIP-2 è§†è§‰é—®ç­”å­¦ä¹ ...")
    
    # æ¶æ„æ¦‚è§ˆ
    blip2_architecture_overview()
    
    # ç³»ç»Ÿæ£€æŸ¥
    if not check_system_requirements():
        print("âŒ ç³»ç»Ÿè¦æ±‚ä¸æ»¡è¶³ï¼Œé€€å‡º")
        return
    
    # è®¾å¤‡æ£€æµ‹
    device = get_device()
    
    # è¯¢é—®æ˜¯å¦ç»§ç»­
    print(f"\nâš ï¸  BLIP-2 æ¨¡å‹è¾ƒå¤§ (~15GB)ï¼Œç¡®è®¤ç»§ç»­ï¼Ÿ")
    try:
        response = input("è¾“å…¥ 'y' ç»§ç»­ï¼Œå…¶ä»–é€€å‡º: ").strip().lower()
        if response != 'y':
            print("ğŸ‘‹ ç”¨æˆ·å–æ¶ˆï¼Œé€€å‡º")
            return
    except (EOFError, KeyboardInterrupt):
        print("\nğŸ‘‹ éäº¤äº’æ¨¡å¼æˆ–ç”¨æˆ·ä¸­æ–­ï¼Œé€€å‡º")
        return
    
    try:
        # åŠ è½½æ¨¡å‹
        model, processor = load_blip2_model(device, "2.7b")
        
        if model is None or processor is None:
            print("âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œé€€å‡º")
            return
        
        # å›¾åƒæè¿°æ¼”ç¤º
        image_captioning_demo(model, processor, device)
        
        # è§†è§‰é—®ç­”æ¼”ç¤º
        visual_question_answering_demo(model, processor, device)
        
        # å¤šè½®å¯¹è¯æ¼”ç¤º
        conversational_vqa_demo(model, processor, device)
        
        # æ€§èƒ½åˆ†æ
        performance_analysis(model, processor, device)
        
        print("\n" + "=" * 60)
        print("âœ… 9.4 BLIP-2 è§†è§‰é—®ç­”å­¦ä¹ å®Œæˆ!")
        print("=" * 60)
        print("\nğŸ¯ å…³é”®æ”¶è·:")
        print("  â€¢ BLIP-2 ä¸‰é˜¶æ®µæ¶æ„è®¾è®¡")
        print("  â€¢ Q-Former çš„æ¡¥æ¢ä½œç”¨")
        print("  â€¢ å¼ºå¤§çš„å›¾åƒç†è§£å’Œç”Ÿæˆèƒ½åŠ›")
        print("  â€¢ å¤šè½®å¯¹è¯çš„å®ç°æ–¹å¼")
        print("\nä¸‹ä¸€æ­¥: è¿è¡Œ 9.5_lightweight_vlm.py å­¦ä¹ è½»é‡çº§æ›¿ä»£æ–¹æ¡ˆ")
        
    except Exception as e:
        print(f"\nâŒ è¿è¡Œå‡ºé”™: {e}")
        print("è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥ã€å†…å­˜å’Œä¾èµ–å®‰è£…")
    
    finally:
        # æ¸…ç†å†…å­˜
        cleanup_memory()


if __name__ == "__main__":
    main()