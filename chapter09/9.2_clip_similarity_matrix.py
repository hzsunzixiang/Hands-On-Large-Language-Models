import os
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

"""
9.2 CLIP ç›¸ä¼¼åº¦çŸ©é˜µåˆ†æ
=========================

æœ¬èŠ‚å†…å®¹:
- å¤šå›¾åƒä¸å¤šæ–‡æœ¬çš„ç›¸ä¼¼åº¦è®¡ç®—
- ç›¸ä¼¼åº¦çŸ©é˜µå¯è§†åŒ–
- é›¶æ ·æœ¬åˆ†ç±»åŸç†
- è·¨æ¨¡æ€æ£€ç´¢åº”ç”¨

é€šè¿‡ç›¸ä¼¼åº¦çŸ©é˜µï¼Œæˆ‘ä»¬å¯ä»¥ç†è§£ CLIP å¦‚ä½•åœ¨å›¾æ–‡ä¹‹é—´å»ºç«‹å¯¹åº”å…³ç³»ï¼Œ
è¿™æ˜¯é›¶æ ·æœ¬åˆ†ç±»å’Œè·¨æ¨¡æ€æ£€ç´¢çš„åŸºç¡€ã€‚
"""

import warnings
warnings.filterwarnings("ignore")

import torch
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns
from PIL import Image
from urllib.request import urlopen

# é…ç½® matplotlib ä¸­æ–‡å­—ä½“ (macOS)
plt.rcParams['font.sans-serif'] = ['PingFang SC', 'Heiti TC', 'STHeiti', 'SimHei', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False  # æ­£ç¡®æ˜¾ç¤ºè´Ÿå·


def get_device():
    """è‡ªåŠ¨æ£€æµ‹æœ€ä½³å¯ç”¨è®¾å¤‡"""
    if torch.cuda.is_available():
        device = "cuda"
        device_name = torch.cuda.get_device_name(0)
        print(f"ä½¿ç”¨è®¾å¤‡: CUDA ({device_name})")
    elif torch.backends.mps.is_available():
        device = "mps"
        print("ä½¿ç”¨è®¾å¤‡: MPS (Apple Silicon GPU)")
    else:
        device = "cpu"
        print("ä½¿ç”¨è®¾å¤‡: CPU")
    return device


# ç¤ºä¾‹å›¾ç‰‡å’Œæè¿°
IMAGE_URLS = {
    "puppy": "https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-Language-Models/main/chapter09/images/puppy.png",
    "cat": "https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-Language-Models/main/chapter09/images/cat.png", 
    "car": "https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-Language-Models/main/chapter09/images/car.png",
}

CAPTIONS = [
    "a puppy playing in the snow",
    "a cat sitting comfortably", 
    "a sports car on the road"
]


def load_image_from_url(url):
    """ä» URL åŠ è½½å›¾ç‰‡"""
    return Image.open(urlopen(url)).convert("RGB")


def similarity_matrix_concept():
    """ç›¸ä¼¼åº¦çŸ©é˜µæ¦‚å¿µè§£é‡Š"""
    print("=" * 60)
    print("ç›¸ä¼¼åº¦çŸ©é˜µæ¦‚å¿µ")
    print("=" * 60)
    
    concept = """
ç›¸ä¼¼åº¦çŸ©é˜µæ˜¯ä»€ä¹ˆ?
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ç›¸ä¼¼åº¦çŸ©é˜µæ˜¯ä¸€ä¸ª MÃ—N çš„çŸ©é˜µï¼Œå…¶ä¸­:
â€¢ M = å›¾åƒæ•°é‡
â€¢ N = æ–‡æœ¬æè¿°æ•°é‡  
â€¢ çŸ©é˜µ[i,j] = ç¬¬iå¼ å›¾åƒä¸ç¬¬jä¸ªæ–‡æœ¬çš„ç›¸ä¼¼åº¦

ç¤ºä¾‹ (3Ã—3 çŸ©é˜µ):
                æ–‡æœ¬1      æ–‡æœ¬2      æ–‡æœ¬3
                puppy      cat       car
å›¾åƒ1 puppy    [0.85]     0.12      0.08
å›¾åƒ2 cat       0.15     [0.92]     0.11  
å›¾åƒ3 car       0.09      0.13     [0.88]

ç†æƒ³æƒ…å†µ: å¯¹è§’çº¿å€¼æœ€é«˜ (æ­£ç¡®åŒ¹é…)

åº”ç”¨åœºæ™¯:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

1. é›¶æ ·æœ¬å›¾åƒåˆ†ç±»:
   â€¢ å°†ç±»åˆ«åä½œä¸ºæ–‡æœ¬æè¿°
   â€¢ è®¡ç®—å›¾åƒä¸æ‰€æœ‰ç±»åˆ«çš„ç›¸ä¼¼åº¦
   â€¢ é€‰æ‹©ç›¸ä¼¼åº¦æœ€é«˜çš„ç±»åˆ«

2. å›¾åƒæ£€ç´¢:
   â€¢ ç”¨æ–‡æœ¬æŸ¥è¯¢æè¿°æƒ³è¦çš„å›¾åƒ
   â€¢ è®¡ç®—æŸ¥è¯¢ä¸æ‰€æœ‰å›¾åƒçš„ç›¸ä¼¼åº¦
   â€¢ è¿”å›ç›¸ä¼¼åº¦æœ€é«˜çš„å›¾åƒ

3. æ–‡æœ¬æ£€ç´¢:
   â€¢ ç”¨å›¾åƒæŸ¥è¯¢ç›¸å…³çš„æ–‡æœ¬æè¿°
   â€¢ è®¡ç®—å›¾åƒä¸æ‰€æœ‰æ–‡æœ¬çš„ç›¸ä¼¼åº¦
   â€¢ è¿”å›ç›¸ä¼¼åº¦æœ€é«˜çš„æ–‡æœ¬

4. å¤šæ¨¡æ€æ¨è:
   â€¢ åŸºäºç”¨æˆ·çš„å›¾åƒåå¥½æ¨èç›¸å…³æ–‡æœ¬
   â€¢ åŸºäºç”¨æˆ·çš„æ–‡æœ¬åå¥½æ¨èç›¸å…³å›¾åƒ
"""
    print(concept)


def clip_similarity_matrix_demo(device=None):
    """
    CLIP ç›¸ä¼¼åº¦çŸ©é˜µæ¼”ç¤º
    è®¡ç®—å¤šå¼ å›¾ç‰‡å’Œå¤šä¸ªæè¿°ä¹‹é—´çš„ç›¸ä¼¼åº¦çŸ©é˜µ
    """
    from transformers import CLIPProcessor, CLIPModel
    
    print("\n" + "=" * 60)
    print("9.2 CLIP ç›¸ä¼¼åº¦çŸ©é˜µåˆ†æ")
    print("=" * 60)
    
    if device is None:
        device = get_device()
    
    # 1. åŠ è½½æ¨¡å‹
    print("\n[æ­¥éª¤ 1] åŠ è½½ CLIP æ¨¡å‹...")
    model_id = "openai/clip-vit-base-patch32"
    clip_processor = CLIPProcessor.from_pretrained(model_id)
    model = CLIPModel.from_pretrained(model_id).to(device)
    print(f"âœ“ æ¨¡å‹åŠ è½½å®Œæˆ: {model_id}")
    
    # 2. åŠ è½½å›¾åƒæ•°æ®
    print("\n[æ­¥éª¤ 2] åŠ è½½å›¾åƒæ•°æ®...")
    images = []
    image_names = []
    
    for name, url in IMAGE_URLS.items():
        try:
            img = load_image_from_url(url)
            images.append(img)
            image_names.append(name)
            print(f"âœ“ åŠ è½½å›¾åƒ: {name} ({img.size})")
        except Exception as e:
            print(f"âœ— è·³è¿‡å›¾åƒ {name}: {e}")
    
    print(f"âœ“ æˆåŠŸåŠ è½½ {len(images)} å¼ å›¾åƒ")
    
    # 3. å‡†å¤‡æ–‡æœ¬æè¿°
    print("\n[æ­¥éª¤ 3] å‡†å¤‡æ–‡æœ¬æè¿°...")
    captions = CAPTIONS[:len(images)]  # ç¡®ä¿æ•°é‡åŒ¹é…
    
    for i, caption in enumerate(captions):
        print(f"âœ“ æ–‡æœ¬ {i+1}: '{caption}'")
    
    # 4. æ‰¹é‡è®¡ç®—åµŒå…¥
    print("\n[æ­¥éª¤ 4] æ‰¹é‡è®¡ç®—åµŒå…¥...")
    
    # ä½¿ç”¨ CLIP çš„æ‰¹é‡å¤„ç†åŠŸèƒ½
    inputs = clip_processor(
        text=captions,
        images=images, 
        return_tensors="pt",
        padding=True
    ).to(device)
    
    print(f"âœ“ æ–‡æœ¬è¾“å…¥å½¢çŠ¶: {inputs['input_ids'].shape}")
    print(f"âœ“ å›¾åƒè¾“å…¥å½¢çŠ¶: {inputs['pixel_values'].shape}")
    
    with torch.no_grad():
        outputs = model(**inputs)
        image_embeds = outputs.image_embeds  # [num_images, 512]
        text_embeds = outputs.text_embeds    # [num_texts, 512]
    
    print(f"âœ“ å›¾åƒåµŒå…¥å½¢çŠ¶: {image_embeds.shape}")
    print(f"âœ“ æ–‡æœ¬åµŒå…¥å½¢çŠ¶: {text_embeds.shape}")
    
    # 5. å½’ä¸€åŒ–åµŒå…¥
    print("\n[æ­¥éª¤ 5] å½’ä¸€åŒ–åµŒå…¥...")
    image_embeds_norm = image_embeds / image_embeds.norm(dim=-1, keepdim=True)
    text_embeds_norm = text_embeds / text_embeds.norm(dim=-1, keepdim=True)
    
    print("âœ“ åµŒå…¥å·²å½’ä¸€åŒ– (L2èŒƒæ•° = 1)")
    
    # 6. è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
    print("\n[æ­¥éª¤ 6] è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ...")
    similarity_matrix = (image_embeds_norm @ text_embeds_norm.T).cpu().numpy()
    
    print(f"âœ“ ç›¸ä¼¼åº¦çŸ©é˜µå½¢çŠ¶: {similarity_matrix.shape}")
    print(f"âœ“ ç›¸ä¼¼åº¦èŒƒå›´: [{similarity_matrix.min():.3f}, {similarity_matrix.max():.3f}]")
    
    return similarity_matrix, image_names, captions


def visualize_similarity_matrix(sim_matrix, image_names, captions):
    """å¯è§†åŒ–ç›¸ä¼¼åº¦çŸ©é˜µ"""
    print("\n[æ­¥éª¤ 7] å¯è§†åŒ–ç›¸ä¼¼åº¦çŸ©é˜µ...")
    
    # 1. æ–‡æœ¬è¡¨æ ¼æ˜¾ç¤º
    print("\nğŸ“Š ç›¸ä¼¼åº¦çŸ©é˜µ (å›¾åƒ Ã— æ–‡æœ¬):")
    print("-" * 80)
    
    # è¡¨å¤´
    print(f"{'å›¾åƒ\\æ–‡æœ¬':>12}", end="")
    for i, caption in enumerate(captions):
        short_caption = caption[:20] + "..." if len(caption) > 20 else caption
        print(f"{short_caption:>25}", end="")
    print()
    
    # æ•°æ®è¡Œ
    for i, img_name in enumerate(image_names):
        print(f"{img_name:>12}", end="")
        for j in range(len(captions)):
            value = sim_matrix[i, j]
            # é«˜äº®å¯¹è§’çº¿å…ƒç´ 
            if i == j:
                print(f"    [{value:>6.3f}]    ", end="")
            else:
                print(f"     {value:>6.3f}     ", end="")
        print()
    
    # 2. çƒ­åŠ›å›¾å¯è§†åŒ–
    plt.figure(figsize=(10, 8))
    
    # åˆ›å»ºçƒ­åŠ›å›¾
    ax = sns.heatmap(
        sim_matrix,
        annot=True,
        fmt='.3f',
        cmap='RdYlBu_r',
        xticklabels=[cap[:30] + "..." if len(cap) > 30 else cap for cap in captions],
        yticklabels=image_names,
        cbar_kws={'label': 'ä½™å¼¦ç›¸ä¼¼åº¦'},
        square=True
    )
    
    plt.title('CLIP å›¾æ–‡ç›¸ä¼¼åº¦çŸ©é˜µ', fontsize=16, pad=20)
    plt.xlabel('æ–‡æœ¬æè¿°', fontsize=12)
    plt.ylabel('å›¾åƒ', fontsize=12)
    plt.xticks(rotation=45, ha='right')
    plt.yticks(rotation=0)
    plt.tight_layout()
    
    # ä¿å­˜å›¾ç‰‡
    plt.savefig('/Users/ericksun/workspace/deeplearning/Hands-On-Large-Language-Models/chapter09/similarity_matrix.png', 
                dpi=300, bbox_inches='tight')
    print("âœ“ çƒ­åŠ›å›¾å·²ä¿å­˜: similarity_matrix.png")
    
    # æ˜¾ç¤ºå›¾ç‰‡ (å¦‚æœåœ¨æ”¯æŒçš„ç¯å¢ƒä¸­)
    try:
        plt.show()
    except:
        print("âœ“ çƒ­åŠ›å›¾ç”Ÿæˆå®Œæˆ (æ— æ³•æ˜¾ç¤ºï¼Œä½†å·²ä¿å­˜)")
    
    plt.close()


def analyze_similarity_results(sim_matrix, image_names, captions):
    """åˆ†æç›¸ä¼¼åº¦ç»“æœ"""
    print("\n[æ­¥éª¤ 8] åˆ†æç›¸ä¼¼åº¦ç»“æœ...")
    
    # 1. å¯¹è§’çº¿åˆ†æ (æ­£ç¡®åŒ¹é…)
    diagonal_values = np.diag(sim_matrix)
    print(f"\nğŸ¯ å¯¹è§’çº¿ç›¸ä¼¼åº¦ (æ­£ç¡®åŒ¹é…):")
    for i, (img_name, caption, score) in enumerate(zip(image_names, captions, diagonal_values)):
        print(f"  {img_name} â†” '{caption}': {score:.4f}")
    
    print(f"âœ“ å¹³å‡å¯¹è§’çº¿ç›¸ä¼¼åº¦: {diagonal_values.mean():.4f}")
    
    # 2. æœ€ä½³åŒ¹é…åˆ†æ
    print(f"\nğŸ† æ¯å¼ å›¾åƒçš„æœ€ä½³æ–‡æœ¬åŒ¹é…:")
    for i, img_name in enumerate(image_names):
        best_text_idx = np.argmax(sim_matrix[i])
        best_score = sim_matrix[i, best_text_idx]
        is_correct = (best_text_idx == i)
        
        status = "âœ“ æ­£ç¡®" if is_correct else "âœ— é”™è¯¯"
        print(f"  {img_name}: '{captions[best_text_idx]}' ({best_score:.4f}) {status}")
    
    print(f"\nğŸ† æ¯ä¸ªæ–‡æœ¬çš„æœ€ä½³å›¾åƒåŒ¹é…:")
    for j, caption in enumerate(captions):
        best_img_idx = np.argmax(sim_matrix[:, j])
        best_score = sim_matrix[best_img_idx, j]
        is_correct = (best_img_idx == j)
        
        status = "âœ“ æ­£ç¡®" if is_correct else "âœ— é”™è¯¯"
        print(f"  '{caption}': {image_names[best_img_idx]} ({best_score:.4f}) {status}")
    
    # 3. å‡†ç¡®ç‡è®¡ç®—
    img_to_text_correct = sum(1 for i in range(len(image_names)) 
                             if np.argmax(sim_matrix[i]) == i)
    text_to_img_correct = sum(1 for j in range(len(captions)) 
                             if np.argmax(sim_matrix[:, j]) == j)
    
    img_to_text_acc = img_to_text_correct / len(image_names)
    text_to_img_acc = text_to_img_correct / len(captions)
    
    print(f"\nğŸ“ˆ æ£€ç´¢å‡†ç¡®ç‡:")
    print(f"  å›¾åƒâ†’æ–‡æœ¬: {img_to_text_correct}/{len(image_names)} = {img_to_text_acc:.1%}")
    print(f"  æ–‡æœ¬â†’å›¾åƒ: {text_to_img_correct}/{len(captions)} = {text_to_img_acc:.1%}")
    print(f"  å¹³å‡å‡†ç¡®ç‡: {(img_to_text_acc + text_to_img_acc) / 2:.1%}")
    
    # 4. ç›¸ä¼¼åº¦åˆ†å¸ƒåˆ†æ
    print(f"\nğŸ“Š ç›¸ä¼¼åº¦åˆ†å¸ƒç»Ÿè®¡:")
    print(f"  æœ€é«˜ç›¸ä¼¼åº¦: {sim_matrix.max():.4f}")
    print(f"  æœ€ä½ç›¸ä¼¼åº¦: {sim_matrix.min():.4f}")
    print(f"  å¹³å‡ç›¸ä¼¼åº¦: {sim_matrix.mean():.4f}")
    print(f"  æ ‡å‡†å·®: {sim_matrix.std():.4f}")
    
    return {
        'diagonal_scores': diagonal_values,
        'img_to_text_acc': img_to_text_acc,
        'text_to_img_acc': text_to_img_acc,
        'avg_accuracy': (img_to_text_acc + text_to_img_acc) / 2
    }


def zero_shot_classification_demo(device):
    """é›¶æ ·æœ¬åˆ†ç±»æ¼”ç¤º"""
    print("\n" + "=" * 60)
    print("é›¶æ ·æœ¬åˆ†ç±»æ¼”ç¤º")
    print("=" * 60)
    
    from transformers import CLIPProcessor, CLIPModel
    
    # åŠ è½½æ¨¡å‹
    model_id = "openai/clip-vit-base-patch32"
    clip_processor = CLIPProcessor.from_pretrained(model_id)
    model = CLIPModel.from_pretrained(model_id).to(device)
    
    # å®šä¹‰åˆ†ç±»ç±»åˆ«
    class_names = [
        "dog", "cat", "car", "airplane", "ship",
        "truck", "bird", "horse", "bicycle", "motorcycle"
    ]
    
    # åˆ›å»ºåˆ†ç±»æ¨¡æ¿
    templates = [f"a photo of a {class_name}" for class_name in class_names]
    
    print(f"âœ“ åˆ†ç±»ç±»åˆ«: {len(class_names)} ä¸ª")
    print(f"âœ“ æ¨¡æ¿ç¤ºä¾‹: '{templates[0]}'")
    
    # æµ‹è¯•å›¾åƒ
    test_image = load_image_from_url(IMAGE_URLS["car"])
    
    # è®¡ç®—åµŒå…¥
    inputs = clip_processor(
        text=templates,
        images=[test_image],
        return_tensors="pt",
        padding=True
    ).to(device)
    
    with torch.no_grad():
        outputs = model(**inputs)
        image_embed = outputs.image_embeds[0:1]  # åªæœ‰ä¸€å¼ å›¾
        text_embeds = outputs.text_embeds
    
    # å½’ä¸€åŒ–
    image_embed = image_embed / image_embed.norm(dim=-1, keepdim=True)
    text_embeds = text_embeds / text_embeds.norm(dim=-1, keepdim=True)
    
    # è®¡ç®—ç›¸ä¼¼åº¦
    similarities = (image_embed @ text_embeds.T).cpu().numpy()[0]
    
    # æ’åºç»“æœ
    sorted_indices = np.argsort(similarities)[::-1]
    
    print(f"\nğŸ” é›¶æ ·æœ¬åˆ†ç±»ç»“æœ (æµ‹è¯•å›¾åƒ: car):")
    print("-" * 40)
    for i, idx in enumerate(sorted_indices[:5]):
        class_name = class_names[idx]
        score = similarities[idx]
        confidence = (score + 1) / 2 * 100  # è½¬æ¢ä¸ºç™¾åˆ†æ¯”
        
        if i == 0:
            print(f"ğŸ† {class_name:>12}: {score:.4f} ({confidence:.1f}%)")
        else:
            print(f"   {class_name:>12}: {score:.4f} ({confidence:.1f}%)")
    
    predicted_class = class_names[sorted_indices[0]]
    print(f"\nâœ“ é¢„æµ‹ç±»åˆ«: {predicted_class}")
    print(f"âœ“ ç½®ä¿¡åº¦: {similarities[sorted_indices[0]]:.4f}")


def cross_modal_retrieval_demo(device):
    """è·¨æ¨¡æ€æ£€ç´¢æ¼”ç¤º"""
    print("\n" + "=" * 60)
    print("è·¨æ¨¡æ€æ£€ç´¢æ¼”ç¤º")
    print("=" * 60)
    
    from transformers import CLIPProcessor, CLIPModel
    
    # åŠ è½½æ¨¡å‹
    model_id = "openai/clip-vit-base-patch32"
    clip_processor = CLIPProcessor.from_pretrained(model_id)
    model = CLIPModel.from_pretrained(model_id).to(device)
    
    # å‡†å¤‡å›¾åƒåº“
    images = []
    image_names = []
    for name, url in IMAGE_URLS.items():
        try:
            images.append(load_image_from_url(url))
            image_names.append(name)
        except:
            pass
    
    # æŸ¥è¯¢æ–‡æœ¬
    query_texts = [
        "cute animal in winter",
        "a fluffy cat",
        "fast vehicle",
        "something blue"
    ]
    
    print(f"âœ“ å›¾åƒåº“: {len(images)} å¼ å›¾åƒ")
    print(f"âœ“ æŸ¥è¯¢: {len(query_texts)} ä¸ªæ–‡æœ¬")
    
    # è®¡ç®—å›¾åƒåµŒå…¥
    image_inputs = clip_processor(images=images, return_tensors="pt").to(device)
    with torch.no_grad():
        image_embeds = model.get_image_features(**image_inputs)
        image_embeds = image_embeds / image_embeds.norm(dim=-1, keepdim=True)
    
    print(f"\nğŸ” æ–‡æœ¬æ£€ç´¢å›¾åƒç»“æœ:")
    print("-" * 50)
    
    for query in query_texts:
        # è®¡ç®—æŸ¥è¯¢åµŒå…¥
        text_inputs = clip_processor(text=[query], return_tensors="pt").to(device)
        with torch.no_grad():
            text_embed = model.get_text_features(**text_inputs)
            text_embed = text_embed / text_embed.norm(dim=-1, keepdim=True)
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = (text_embed @ image_embeds.T).cpu().numpy()[0]
        
        # æ‰¾åˆ°æœ€ä½³åŒ¹é…
        best_idx = np.argmax(similarities)
        best_image = image_names[best_idx]
        best_score = similarities[best_idx]
        
        print(f"æŸ¥è¯¢: '{query}'")
        print(f"  â†’ æœ€ä½³åŒ¹é…: {best_image} (ç›¸ä¼¼åº¦: {best_score:.4f})")
        
        # æ˜¾ç¤ºæ‰€æœ‰ç»“æœ
        sorted_indices = np.argsort(similarities)[::-1]
        print(f"  â†’ æ’åºç»“æœ: ", end="")
        for i, idx in enumerate(sorted_indices):
            print(f"{image_names[idx]}({similarities[idx]:.3f})", end="")
            if i < len(sorted_indices) - 1:
                print(", ", end="")
        print("\n")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹ CLIP ç›¸ä¼¼åº¦çŸ©é˜µå­¦ä¹ ...")
    
    # æ¦‚å¿µè§£é‡Š
    similarity_matrix_concept()
    
    # è®¾å¤‡æ£€æµ‹
    device = get_device()
    
    try:
        # ç›¸ä¼¼åº¦çŸ©é˜µæ¼”ç¤º
        sim_matrix, image_names, captions = clip_similarity_matrix_demo(device)
        
        # å¯è§†åŒ–
        visualize_similarity_matrix(sim_matrix, image_names, captions)
        
        # ç»“æœåˆ†æ
        results = analyze_similarity_results(sim_matrix, image_names, captions)
        
        # é›¶æ ·æœ¬åˆ†ç±»æ¼”ç¤º
        zero_shot_classification_demo(device)
        
        # è·¨æ¨¡æ€æ£€ç´¢æ¼”ç¤º
        cross_modal_retrieval_demo(device)
        
        print("\n" + "=" * 60)
        print("âœ… 9.2 CLIP ç›¸ä¼¼åº¦çŸ©é˜µå­¦ä¹ å®Œæˆ!")
        print("=" * 60)
        print(f"ğŸ“Š å…³é”®æŒ‡æ ‡:")
        print(f"  â€¢ å¹³å‡æ£€ç´¢å‡†ç¡®ç‡: {results['avg_accuracy']:.1%}")
        print(f"  â€¢ å¯¹è§’çº¿å¹³å‡ç›¸ä¼¼åº¦: {results['diagonal_scores'].mean():.4f}")
        print("\nä¸‹ä¸€æ­¥: è¿è¡Œ 9.3_sbert_clip.py å­¦ä¹ ç®€åŒ–æ¥å£")
        
    except Exception as e:
        print(f"\nâŒ è¿è¡Œå‡ºé”™: {e}")
        print("è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œä¾èµ–å®‰è£…")


if __name__ == "__main__":
    main()