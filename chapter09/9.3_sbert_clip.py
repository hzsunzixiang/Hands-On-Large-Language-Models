"""
9.3 SBERT-CLIP ç®€åŒ–æ¥å£
========================

æœ¬èŠ‚å†…å®¹:
- Sentence-Transformers åº“çš„ CLIP å°è£…
- ç»Ÿä¸€çš„ç¼–ç æ¥å£
- æ‰¹é‡å¤„ç†å’Œç›¸ä¼¼åº¦è®¡ç®—
- å®ç”¨å·¥å…·å‡½æ•°

Sentence-Transformers æä¾›äº†æ›´ç®€æ´çš„ API æ¥ä½¿ç”¨ CLIPï¼Œ
è®©å¤šæ¨¡æ€åµŒå…¥çš„ä½¿ç”¨å˜å¾—æ›´åŠ ä¾¿æ·ã€‚
"""

import warnings
warnings.filterwarnings("ignore")

import torch
import numpy as np
from PIL import Image
from urllib.request import urlopen
import matplotlib.pyplot as plt


def get_device():
    """è‡ªåŠ¨æ£€æµ‹æœ€ä½³å¯ç”¨è®¾å¤‡"""
    if torch.cuda.is_available():
        device = "cuda"
        device_name = torch.cuda.get_device_name(0)
        print(f"ä½¿ç”¨è®¾å¤‡: CUDA ({device_name})")
    elif torch.backends.mps.is_available():
        device = "mps"
        print("ä½¿ç”¨è®¾å¤‡: MPS (Apple Silicon GPU)")
    else:
        device = "cpu"
        print("ä½¿ç”¨è®¾å¤‡: CPU")
    return device


# ç¤ºä¾‹æ•°æ®
IMAGE_URLS = {
    "puppy": "https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-Language-Models/main/chapter09/images/puppy.png",
    "beach": "https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-Language-Models/main/chapter09/images/beach.png",
    "car": "https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-Language-Models/main/chapter09/images/car.png",
}


def load_image_from_url(url):
    """ä» URL åŠ è½½å›¾ç‰‡"""
    return Image.open(urlopen(url)).convert("RGB")


def sbert_clip_overview():
    """SBERT-CLIP æ¦‚è§ˆ"""
    print("=" * 60)
    print("SBERT-CLIP æ¦‚è§ˆ")
    print("=" * 60)
    
    overview = """
Sentence-Transformers CLIP å°è£…çš„ä¼˜åŠ¿:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

1. ç»Ÿä¸€æ¥å£:
   âœ“ model.encode() - ç»Ÿä¸€ç¼–ç å›¾åƒå’Œæ–‡æœ¬
   âœ“ ä¸æ–‡æœ¬åµŒå…¥æ¨¡å‹ API ä¸€è‡´
   âœ“ æ— éœ€åˆ†åˆ«å¤„ç†å›¾åƒå’Œæ–‡æœ¬

2. ä¾¿æ·å·¥å…·:
   âœ“ util.cos_sim() - ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—
   âœ“ util.semantic_search() - è¯­ä¹‰æœç´¢
   âœ“ util.paraphrase_mining() - é‡Šä¹‰æŒ–æ˜

3. æ‰¹é‡å¤„ç†:
   âœ“ è‡ªåŠ¨æ‰¹å¤„ç†ä¼˜åŒ–
   âœ“ å†…å­˜ç®¡ç†
   âœ“ GPU åŠ é€Ÿæ”¯æŒ

4. å¤šç§æ¨¡å‹:
   âœ“ clip-ViT-B-32 (æ ‡å‡†ç‰ˆ)
   âœ“ clip-ViT-L-14 (å¤§æ¨¡å‹)
   âœ“ multilingual-clip (å¤šè¯­è¨€)

API å¯¹æ¯”:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

åŸå§‹ Transformers API:
```python
# éœ€è¦åˆ†åˆ«å¤„ç†
tokenizer = CLIPTokenizer.from_pretrained(model_id)
processor = CLIPProcessor.from_pretrained(model_id)
model = CLIPModel.from_pretrained(model_id)

text_inputs = tokenizer(texts, return_tensors="pt")
image_inputs = processor(images=images, return_tensors="pt")

text_embeds = model.get_text_features(**text_inputs)
image_embeds = model.get_image_features(**image_inputs)
```

SBERT-CLIP API:
```python
# ç»Ÿä¸€å¤„ç†
model = SentenceTransformer('clip-ViT-B-32')

text_embeds = model.encode(texts)
image_embeds = model.encode(images)
```

æ›´ç®€æ´ï¼Œæ›´æ˜“ç”¨ï¼
"""
    print(overview)


def sbert_clip_basic_demo(device=None):
    """
    SBERT-CLIP åŸºç¡€æ¼”ç¤º
    å±•ç¤ºç»Ÿä¸€çš„ç¼–ç æ¥å£
    """
    from sentence_transformers import SentenceTransformer, util
    
    print("\n" + "=" * 60)
    print("9.3 SBERT-CLIP åŸºç¡€æ¼”ç¤º")
    print("=" * 60)
    
    if device is None:
        device = get_device()
    
    # 1. åŠ è½½æ¨¡å‹
    print("\n[æ­¥éª¤ 1] åŠ è½½ SBERT-CLIP æ¨¡å‹...")
    model_name = 'clip-ViT-B-32'
    model = SentenceTransformer(model_name, device=device)
    
    print(f"âœ“ æ¨¡å‹: {model_name}")
    print(f"âœ“ è®¾å¤‡: {device}")
    print(f"âœ“ åµŒå…¥ç»´åº¦: {model.get_sentence_embedding_dimension()}")
    
    # 2. å‡†å¤‡æ•°æ®
    print("\n[æ­¥éª¤ 2] å‡†å¤‡æµ‹è¯•æ•°æ®...")
    
    # åŠ è½½å›¾åƒ
    images = []
    image_names = []
    for name, url in IMAGE_URLS.items():
        try:
            img = load_image_from_url(url)
            images.append(img)
            image_names.append(name)
            print(f"âœ“ å›¾åƒ: {name}")
        except Exception as e:
            print(f"âœ— è·³è¿‡: {name} - {e}")
    
    # å‡†å¤‡æ–‡æœ¬
    texts = [
        "a puppy playing in the snow",
        "a sandy beach with ocean waves", 
        "a sports car on the road"
    ]
    
    print(f"âœ“ æ–‡æœ¬æ•°é‡: {len(texts)}")
    
    # 3. ç»Ÿä¸€ç¼–ç 
    print("\n[æ­¥éª¤ 3] ç»Ÿä¸€ç¼–ç ...")
    
    # ç¼–ç å›¾åƒ (è‡ªåŠ¨æ‰¹å¤„ç†)
    print("  ç¼–ç å›¾åƒ...")
    image_embeddings = model.encode(images, convert_to_tensor=True)
    print(f"âœ“ å›¾åƒåµŒå…¥å½¢çŠ¶: {image_embeddings.shape}")
    
    # ç¼–ç æ–‡æœ¬
    print("  ç¼–ç æ–‡æœ¬...")
    text_embeddings = model.encode(texts, convert_to_tensor=True)
    print(f"âœ“ æ–‡æœ¬åµŒå…¥å½¢çŠ¶: {text_embeddings.shape}")
    
    # 4. ç›¸ä¼¼åº¦è®¡ç®—
    print("\n[æ­¥éª¤ 4] è®¡ç®—ç›¸ä¼¼åº¦...")
    
    # ä½¿ç”¨ util.cos_sim() è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
    similarity_matrix = util.cos_sim(image_embeddings, text_embeddings)
    
    print(f"âœ“ ç›¸ä¼¼åº¦çŸ©é˜µå½¢çŠ¶: {similarity_matrix.shape}")
    print(f"âœ“ æ•°æ®ç±»å‹: {type(similarity_matrix)}")
    
    # 5. ç»“æœå±•ç¤º
    print("\n[æ­¥éª¤ 5] ç»“æœå±•ç¤º...")
    
    sim_np = similarity_matrix.cpu().numpy()
    
    print("\nğŸ“Š ç›¸ä¼¼åº¦çŸ©é˜µ:")
    print("-" * 70)
    print(f"{'å›¾åƒ\\æ–‡æœ¬':>10}", end="")
    for text in texts:
        short_text = text[:20] + "..." if len(text) > 20 else text
        print(f"{short_text:>25}", end="")
    print()
    
    for i, img_name in enumerate(image_names):
        print(f"{img_name:>10}", end="")
        for j in range(len(texts)):
            value = sim_np[i, j]
            if i == j:  # å¯¹è§’çº¿é«˜äº®
                print(f"    [{value:>6.3f}]    ", end="")
            else:
                print(f"     {value:>6.3f}     ", end="")
        print()
    
    return model, similarity_matrix


def advanced_similarity_operations(model, device):
    """é«˜çº§ç›¸ä¼¼åº¦æ“ä½œ"""
    print("\n" + "=" * 60)
    print("é«˜çº§ç›¸ä¼¼åº¦æ“ä½œ")
    print("=" * 60)
    
    from sentence_transformers import util
    
    # 1. è¯­ä¹‰æœç´¢æ¼”ç¤º
    print("\n[æ“ä½œ 1] è¯­ä¹‰æœç´¢...")
    
    # å‡†å¤‡å›¾åƒåº“
    images = []
    image_descriptions = []
    for name, url in IMAGE_URLS.items():
        try:
            img = load_image_from_url(url)
            images.append(img)
            image_descriptions.append(f"{name} image")
        except:
            pass
    
    # ç¼–ç å›¾åƒåº“
    image_embeddings = model.encode(images)
    
    # æœç´¢æŸ¥è¯¢
    queries = [
        "cute animal",
        "nature landscape", 
        "transportation vehicle",
        "winter scene"
    ]
    
    print(f"âœ“ å›¾åƒåº“: {len(images)} å¼ ")
    print(f"âœ“ æŸ¥è¯¢: {len(queries)} ä¸ª")
    
    for query in queries:
        # ç¼–ç æŸ¥è¯¢
        query_embedding = model.encode([query])
        
        # è¯­ä¹‰æœç´¢
        search_results = util.semantic_search(
            query_embedding, 
            image_embeddings, 
            top_k=len(images)
        )[0]
        
        print(f"\nğŸ” æŸ¥è¯¢: '{query}'")
        for i, result in enumerate(search_results):
            idx = result['corpus_id']
            score = result['score']
            desc = image_descriptions[idx]
            print(f"  {i+1}. {desc}: {score:.4f}")
    
    # 2. æ‰¹é‡ç›¸ä¼¼åº¦è®¡ç®—
    print(f"\n[æ“ä½œ 2] æ‰¹é‡ç›¸ä¼¼åº¦è®¡ç®—...")
    
    # åˆ›å»ºæ›´å¤šæµ‹è¯•æ–‡æœ¬
    extended_texts = [
        "a dog playing outside",
        "puppy in snow",
        "ocean waves",
        "beach vacation",
        "red sports car",
        "fast vehicle",
        "mountain landscape",
        "city street"
    ]
    
    # æ‰¹é‡ç¼–ç 
    text_embeddings = model.encode(extended_texts)
    
    # è®¡ç®—å›¾åƒä¸æ‰€æœ‰æ–‡æœ¬çš„ç›¸ä¼¼åº¦
    all_similarities = util.cos_sim(image_embeddings, text_embeddings)
    
    print(f"âœ“ æ‰©å±•ç›¸ä¼¼åº¦çŸ©é˜µ: {all_similarities.shape}")
    
    # æ‰¾åˆ°æ¯å¼ å›¾åƒçš„æœ€ä½³æ–‡æœ¬åŒ¹é…
    for i, img_desc in enumerate(image_descriptions):
        similarities = all_similarities[i].cpu().numpy()
        best_indices = np.argsort(similarities)[::-1][:3]
        
        print(f"\nğŸ† {img_desc} çš„æœ€ä½³åŒ¹é…:")
        for j, idx in enumerate(best_indices):
            text = extended_texts[idx]
            score = similarities[idx]
            print(f"  {j+1}. '{text}': {score:.4f}")
    
    return all_similarities


def clustering_and_visualization(model, device):
    """èšç±»å’Œå¯è§†åŒ–"""
    print("\n" + "=" * 60)
    print("èšç±»å’Œå¯è§†åŒ–")
    print("=" * 60)
    
    from sentence_transformers import util
    from sklearn.decomposition import PCA
    from sklearn.cluster import KMeans
    import matplotlib.pyplot as plt
    
    # 1. å‡†å¤‡æ··åˆæ•°æ®
    print("\n[æ­¥éª¤ 1] å‡†å¤‡æ··åˆæ•°æ®...")
    
    # å›¾åƒæ•°æ®
    images = []
    image_labels = []
    for name, url in IMAGE_URLS.items():
        try:
            img = load_image_from_url(url)
            images.append(img)
            image_labels.append(f"img_{name}")
        except:
            pass
    
    # æ–‡æœ¬æ•°æ®
    texts = [
        "cute puppy playing",
        "dog in winter",
        "beautiful beach scene",
        "ocean and sand",
        "sports car racing",
        "red vehicle"
    ]
    text_labels = [f"text_{i}" for i in range(len(texts))]
    
    # 2. ç¼–ç æ‰€æœ‰æ•°æ®
    print("\n[æ­¥éª¤ 2] ç¼–ç æ··åˆæ•°æ®...")
    
    image_embeddings = model.encode(images)
    text_embeddings = model.encode(texts)
    
    # åˆå¹¶åµŒå…¥
    all_embeddings = np.vstack([image_embeddings, text_embeddings])
    all_labels = image_labels + text_labels
    
    print(f"âœ“ æ€»åµŒå…¥æ•°é‡: {all_embeddings.shape[0]}")
    print(f"âœ“ åµŒå…¥ç»´åº¦: {all_embeddings.shape[1]}")
    
    # 3. é™ç»´å¯è§†åŒ–
    print("\n[æ­¥éª¤ 3] PCA é™ç»´...")
    
    pca = PCA(n_components=2)
    embeddings_2d = pca.fit_transform(all_embeddings)
    
    print(f"âœ“ é™ç»´åå½¢çŠ¶: {embeddings_2d.shape}")
    print(f"âœ“ è§£é‡Šæ–¹å·®æ¯”: {pca.explained_variance_ratio_}")
    
    # 4. èšç±»åˆ†æ
    print("\n[æ­¥éª¤ 4] K-means èšç±»...")
    
    n_clusters = 3
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    cluster_labels = kmeans.fit_predict(all_embeddings)
    
    print(f"âœ“ èšç±»æ•°é‡: {n_clusters}")
    
    # 5. å¯è§†åŒ–
    print("\n[æ­¥éª¤ 5] ç”Ÿæˆå¯è§†åŒ–...")
    
    plt.figure(figsize=(12, 8))
    
    # ç»˜åˆ¶èšç±»ç»“æœ
    colors = ['red', 'blue', 'green', 'orange', 'purple']
    markers = ['o', 's', '^', 'D', 'v']
    
    for i in range(n_clusters):
        cluster_mask = cluster_labels == i
        plt.scatter(
            embeddings_2d[cluster_mask, 0],
            embeddings_2d[cluster_mask, 1],
            c=colors[i % len(colors)],
            marker=markers[i % len(markers)],
            s=100,
            alpha=0.7,
            label=f'Cluster {i}'
        )
    
    # æ·»åŠ æ ‡ç­¾
    for i, (x, y) in enumerate(embeddings_2d):
        label = all_labels[i]
        plt.annotate(
            label,
            (x, y),
            xytext=(5, 5),
            textcoords='offset points',
            fontsize=8,
            alpha=0.8
        )
    
    plt.title('CLIP åµŒå…¥ç©ºé—´å¯è§†åŒ– (PCAé™ç»´)', fontsize=14)
    plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')
    plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    
    # ä¿å­˜å›¾ç‰‡
    plt.savefig('/Users/ericksun/workspace/deeplearning/Hands-On-Large-Language-Models/chapter09/clip_embedding_visualization.png',
                dpi=300, bbox_inches='tight')
    print("âœ“ å¯è§†åŒ–å·²ä¿å­˜: clip_embedding_visualization.png")
    
    try:
        plt.show()
    except:
        print("âœ“ å¯è§†åŒ–ç”Ÿæˆå®Œæˆ")
    
    plt.close()
    
    # 6. èšç±»åˆ†æ
    print("\n[æ­¥éª¤ 6] èšç±»ç»“æœåˆ†æ...")
    
    for i in range(n_clusters):
        cluster_items = [all_labels[j] for j in range(len(all_labels)) if cluster_labels[j] == i]
        print(f"\nğŸ” Cluster {i}: {cluster_items}")


def practical_applications(model, device):
    """å®é™…åº”ç”¨ç¤ºä¾‹"""
    print("\n" + "=" * 60)
    print("å®é™…åº”ç”¨ç¤ºä¾‹")
    print("=" * 60)
    
    from sentence_transformers import util
    
    # 1. å›¾åƒæ ‡æ³¨ç”Ÿæˆ
    print("\n[åº”ç”¨ 1] è‡ªåŠ¨å›¾åƒæ ‡æ³¨...")
    
    # é¢„å®šä¹‰æ ‡ç­¾åº“
    label_categories = {
        "animals": ["dog", "cat", "puppy", "kitten", "pet", "animal"],
        "nature": ["beach", "ocean", "sea", "sand", "water", "landscape"],
        "vehicles": ["car", "automobile", "vehicle", "transportation", "sports car"],
        "weather": ["snow", "winter", "cold", "sunny", "cloudy"],
        "activities": ["playing", "running", "sleeping", "driving", "swimming"]
    }
    
    # å±•å¹³æ‰€æœ‰æ ‡ç­¾
    all_labels = []
    for category, labels in label_categories.items():
        all_labels.extend(labels)
    
    # ç¼–ç æ ‡ç­¾
    label_embeddings = model.encode(all_labels)
    
    # ä¸ºæ¯å¼ å›¾åƒç”Ÿæˆæ ‡æ³¨
    for name, url in IMAGE_URLS.items():
        try:
            image = load_image_from_url(url)
            image_embedding = model.encode([image])
            
            # è®¡ç®—ä¸æ‰€æœ‰æ ‡ç­¾çš„ç›¸ä¼¼åº¦
            similarities = util.cos_sim(image_embedding, label_embeddings)[0]
            
            # è·å–top-5æ ‡ç­¾
            top_indices = similarities.argsort(descending=True)[:5]
            
            print(f"\nğŸ·ï¸  {name} å›¾åƒçš„è‡ªåŠ¨æ ‡æ³¨:")
            for i, idx in enumerate(top_indices):
                label = all_labels[idx]
                score = similarities[idx].item()
                print(f"  {i+1}. {label}: {score:.4f}")
                
        except Exception as e:
            print(f"âœ— è·³è¿‡ {name}: {e}")
    
    # 2. å†…å®¹æ¨èç³»ç»Ÿ
    print(f"\n[åº”ç”¨ 2] åŸºäºå›¾åƒçš„å†…å®¹æ¨è...")
    
    # æ¨¡æ‹Ÿç”¨æˆ·åå¥½
    user_preferences = [
        "I love cute animals",
        "I enjoy beach vacations",
        "I'm interested in fast cars"
    ]
    
    # ç¼–ç ç”¨æˆ·åå¥½
    preference_embeddings = model.encode(user_preferences)
    
    # å›¾åƒåº“
    images = []
    image_names = []
    for name, url in IMAGE_URLS.items():
        try:
            img = load_image_from_url(url)
            images.append(img)
            image_names.append(name)
        except:
            pass
    
    image_embeddings = model.encode(images)
    
    # ä¸ºæ¯ä¸ªç”¨æˆ·åå¥½æ¨èå›¾åƒ
    for i, preference in enumerate(user_preferences):
        similarities = util.cos_sim(preference_embeddings[i:i+1], image_embeddings)[0]
        best_idx = similarities.argmax()
        best_score = similarities[best_idx].item()
        
        print(f"\nğŸ‘¤ ç”¨æˆ·åå¥½: '{preference}'")
        print(f"   æ¨èå›¾åƒ: {image_names[best_idx]} (ç›¸ä¼¼åº¦: {best_score:.4f})")
        
        # æ˜¾ç¤ºæ‰€æœ‰åŒ¹é…åº¦
        sorted_indices = similarities.argsort(descending=True)
        print(f"   å®Œæ•´æ’åº: ", end="")
        for j, idx in enumerate(sorted_indices):
            print(f"{image_names[idx]}({similarities[idx]:.3f})", end="")
            if j < len(sorted_indices) - 1:
                print(", ", end="")
        print()
    
    # 3. å¤šæ¨¡æ€æœç´¢å¼•æ“
    print(f"\n[åº”ç”¨ 3] å¤šæ¨¡æ€æœç´¢å¼•æ“...")
    
    # æ„å»ºæ··åˆç´¢å¼• (å›¾åƒ + æ–‡æœ¬)
    search_corpus = []
    corpus_types = []
    corpus_items = []
    
    # æ·»åŠ å›¾åƒ
    for name, url in IMAGE_URLS.items():
        try:
            img = load_image_from_url(url)
            search_corpus.append(img)
            corpus_types.append("image")
            corpus_items.append(f"image_{name}")
        except:
            pass
    
    # æ·»åŠ æ–‡æœ¬æè¿°
    text_descriptions = [
        "A cute puppy playing in the snow during winter",
        "Beautiful sandy beach with clear blue ocean waves",
        "Red sports car driving on an empty road"
    ]
    
    search_corpus.extend(text_descriptions)
    corpus_types.extend(["text"] * len(text_descriptions))
    corpus_items.extend([f"text_{i}" for i in range(len(text_descriptions))])
    
    # ç¼–ç æ•´ä¸ªè¯­æ–™åº“
    corpus_embeddings = model.encode(search_corpus)
    
    # æœç´¢æŸ¥è¯¢
    search_queries = [
        "winter animals",
        "vacation destination",
        "fast transportation"
    ]
    
    print(f"âœ“ æœç´¢è¯­æ–™åº“: {len(search_corpus)} é¡¹")
    print(f"  - å›¾åƒ: {corpus_types.count('image')} ä¸ª")
    print(f"  - æ–‡æœ¬: {corpus_types.count('text')} ä¸ª")
    
    for query in search_queries:
        query_embedding = model.encode([query])
        
        # æœç´¢æœ€ç›¸å…³çš„å†…å®¹
        search_results = util.semantic_search(
            query_embedding,
            corpus_embeddings,
            top_k=3
        )[0]
        
        print(f"\nğŸ” æœç´¢: '{query}'")
        for j, result in enumerate(search_results):
            idx = result['corpus_id']
            score = result['score']
            item_type = corpus_types[idx]
            item_name = corpus_items[idx]
            
            print(f"  {j+1}. [{item_type}] {item_name}: {score:.4f}")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹ SBERT-CLIP å­¦ä¹ ...")
    
    # æ¦‚è§ˆ
    sbert_clip_overview()
    
    # è®¾å¤‡æ£€æµ‹
    device = get_device()
    
    try:
        # åŸºç¡€æ¼”ç¤º
        model, sim_matrix = sbert_clip_basic_demo(device)
        
        # é«˜çº§æ“ä½œ
        advanced_similarity_operations(model, device)
        
        # èšç±»å’Œå¯è§†åŒ–
        clustering_and_visualization(model, device)
        
        # å®é™…åº”ç”¨
        practical_applications(model, device)
        
        print("\n" + "=" * 60)
        print("âœ… 9.3 SBERT-CLIP å­¦ä¹ å®Œæˆ!")
        print("=" * 60)
        print("\nğŸ¯ å…³é”®æ”¶è·:")
        print("  â€¢ ç»Ÿä¸€çš„å¤šæ¨¡æ€ç¼–ç æ¥å£")
        print("  â€¢ ä¾¿æ·çš„ç›¸ä¼¼åº¦è®¡ç®—å·¥å…·")
        print("  â€¢ ä¸°å¯Œçš„å®é™…åº”ç”¨åœºæ™¯")
        print("  â€¢ é«˜æ•ˆçš„æ‰¹é‡å¤„ç†èƒ½åŠ›")
        print("\nä¸‹ä¸€æ­¥: è¿è¡Œ 9.4_blip2_vision_qa.py å­¦ä¹ è§†è§‰é—®ç­”")
        
    except Exception as e:
        print(f"\nâŒ è¿è¡Œå‡ºé”™: {e}")
        print("è¯·æ£€æŸ¥ sentence-transformers åº“æ˜¯å¦æ­£ç¡®å®‰è£…")
        print("å®‰è£…å‘½ä»¤: pip install sentence-transformers")


if __name__ == "__main__":
    main()